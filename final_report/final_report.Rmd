---
title: 'Reproducibility Report for: The Forecasting Collaborative. Insights into the
  accuracy of social scientistsâ€™ forecasts of societal change'
author: "Ana Martinovici, Ekaterina Pronizius, Erin M. Buchanan (order in report)"
date: "Last compiled on `r Sys.time()`"
output: 
    bookdown::html_document2:
        toc: true
        css: !expr here::here("final_report", "style.css")
        toc_float: 
            collapsed: false
        number_sections: true
        code_folding: show
        theme: cerulean
editor_options: 
  chunk_output_type: console
---

For the original version of our pre-re-analysis plan, see [Link to the original re-analysis plan](https://github.com/Meta-Science-Replication-Team/repro-Forecasting_Tournament/blob/c184756586011cb0b9f8283a771e6a44db74861c/Replication_AnaErinKatja/replication_report.Rmd). In this report, original wording of our re-analysis plan is included in highlighted **orange** infoboxes, and the original wording of the pre-registration from the authors is present in **blue** infoboxes. Note that infoboxes are displayed only in the html output (not in pdf).

::: infobox
The article aims to answer "How well can social scientists predict societal change, and what processes underlie their predictions?" (abstract, page 484). Our robustness reproduction will focus on a subset of the analyses reported in the paper. Specifically, we will focus on results and claims reported in the section "How accurate were behavioural and social scientists at forecasting?" (pages 485-486). For this section, we plan to:
:::

# Computational Reproducibility

```{r, child=here::here("comp_repro_check", "comp_repro_check.Rmd")}
```

# Preregistration

::: infobox
2.  check the match between the preregistration plan of the paper (submitted by the authors) and the final paper
:::

```{r, child=here::here("pre_reg_check", "prereg_check.Rmd")}
```

# Robustness 1: MAPE instead of MASE

::: infobox
3.  check robustness reproducibility by using a different operationalization of forecasting accuracy (i.e., the DV). Instead of MASE, we will use MAPE (mean absolute percent error) and use random effects for domain.
:::

Given the results from the reproducibility check and the results for Robustness 3 test, we did not complete this test. 

# Robustness 2: sensitivity to forecast size

::: infobox
4.  check robustness reproducibility by examining how sensitive the results are when accounting for forecast size (i.e., the number of claims that participating teams self-selected into submitting).
:::

Given the results from the reproducibility check and the results for Robustness 3 test, we did not complete this test. 

# Robustness 3: Comparison to judgments of learning literature

::: infobox
5.  perform a robustness comparison to judgements of learning literature. We will calculate bias and sensitivity of the forecasting for each team by domain combination, by using a regression equation of `actual answer ~ real answer` and then extracting estimated intercept (bias) and slope (sensitivity) for each team and domain. Then, we can estimate an MLM (multilevel model) of `bias ~ 1 + (1|domain)` to determine if bias is different from zero and an MLM of `sensitivity ~ 1 + (1|domain)` to determine if sensitivity is different from zero. If these are different from zero, the forecasts are "biased" and "sensitive". Bias is traditionally .4-.6 on a standardized scale range - any values outside this range would be considered different from traditional results. Sensitivity is traditionally .2 to .4 on a standardized scale range - any values outside this range would be considered different from traditional results.
:::

```{r, child=here::here("jol_test", "jol_extension.Rmd")}
```

# Session Info

```{r}
sessionInfo()
```


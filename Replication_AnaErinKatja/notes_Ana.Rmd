---
title: "Notes and screaming in the void"
author: "Ana Martinovici"
date: "Last compiled on `r Sys.time()`"
output: 
    bookdown::html_document2:
        toc: true
        css: style.css
        toc_float: 
            collapsed: false
        number_sections: true
        code_folding: show
        theme: cerulean
editor_options: 
  chunk_output_type: console
---

This is the doc where I take notes as I read the paper or work on reproducing the results.

# Notes on the paper

The core RQ of the paper is "How well can social scientists predict societal change, and what processes underlie their predictions?".

To check:

-   do people predict "change" or do they predict the level?

# Follow-up on our Pre-re-analysis plan

For the original version of the pre-re-analysis plan, see [add link here]

The article aims to answer "How well can social scientists predict societal change, and what processes underlie their predictions?" (abstract, page 484). Our robustness reproduction will focus on a subset of the analyses reported in the paper. Specifically, we will focus on results and claims reported in the section "How accurate were behavioural and social scientists at forecasting?" (pages 485-486). For this section, we plan to:

1.  check computational reproducibility using the data and code provided by the authors. We expect that we have to make changes in order to execute the analysis code, and we aim to many as few changes as possible.

We planed to spend at most 12 hours on this step (3 sessions of max 4 hours, spread over several days). Within the team we have extensive experience with R, statistics, and good practices for computational reproducibility. We think that 12 hours of work is a very high amount of effort, and ideally readers should be able to reproduce results in a much shorter amount of time (\<2 hours). We will report the amount of time that we spend and the changes we have to make to the existing code.

The results were not computationally reproducible within the planned time. This is due to how data, code, and instructions are provided by the authors in the reproducibility package. We list below specific issues and ways to improve them:

-   the README file of the repository provides incomplete information about which files and how should be used to reproduce the results in the paper. Not all the files that seem to contain data based on their name are described in the README file (e.g., `Wave1+2data.csv`, `Wave1+2demographics.csv`, `contrast1.csv`)

The repository contains 124 files: 44 in the root directory and the rest in one of 5 subdirectories. 

- some files are duplicates (i.e., have the same name and same content) and it is unclear why and which one should be used. For example: `BenchmarkSimulations.R` is both in the root directory and the `sim` directory.

- some files have the same name yet different content

`dat_long.csv` is both in the root directory (V1) and the `Data Cleaning` directory (V2). The README file describes it as "`dat_long.csv` contains a file with predictions of forecasting teams in a long format, used for plotting estimates of each team." I have checked if the two files have the same content and they do not. V1 has 26960 observations and 144 variables, while V2 has 27032 observations and 134 variables. The two datasets share 131 variables. I checked if there are differences in the observations for these shared 131 variables: of the 26960 observations in V1, 477 observations match V2. Given the size (observations and variables) of these datasets, and the limited information available in the repo, it is not feasible to conduct further checks. 

`dat_long.csv` is generated by `Wave 1+2 Descriptives.Rmd`. It is not clear which of the two files, if any, is generated by the Rmd (this depends on the knit directory settings of the user who last knitted this file). The Rmd file contains code that sets a working directory to `setwd("~/GitHub/Forecasting-Tournament") #igor's working directory`. Based on this, it is more likely that the csv file is saved in the root directory. After commenting out the `setwd` command, I tried to knit the file. The file could then be knit without error and two files were generated `dat_for_analyses.csv` and `dat_long.csv`. I will refer to the `dat_long.csv` generated after I knit the Rmd as V3. V3 has 26960 observations (same as V1) and 138 variables (different from either V1 or V2). It is thus unclear how the dataset that contains predictions of forecasting teams was generated. This is a major issue that prevents further checks for computational reproducibility. 

- unclear which of the 2 Analyses Rmd files should be used. 

  - `Wave 1+2 Analyses FINAL FOR MANUSCRIPT.Rmd`
  
The file cannot be knit without error, even after installing all packages and commenting out the `setwd` line of code. "Quitting from lines 1149-1372 [Phases 1 and 2 along with sims] (Wave-1+2-Analyses-FINAL-FOR-MANUSCRIPT.Rmd)". I have checked these lines of code, and there is a mistake in how arguments to the `stan_lmer` function are provided. A parenthesis is missing, which means that this Rmd file was not knitted by the authors, so probably this is not the file that was used to generate the results. Trying to knit the file, overwrote 28 of the 29 files in `Wave-1+2-Analyses-FINAL-FOR-MANUSCRIPT_files\figure-html`. These are all image files for plots. Some of these plots are substantively different (i.e., the original version shared by authors is different from what is generated when knitting the Rmd file). These differences are important (e.g., different number of variables being plotted), so these cannot be explained by differences in resolution or other display settings between the device of the authors and the device of the user trying to reproduce results. 
In conclusion, this version of the file could not have been used to generate the results in the paper.

  - `Wave 1+2 Analyses update.Rmd`
  
Next, I tried to knit this file. After commenting out the `setwd` line, the file could not be knit without error.

"Quitting from lines 747-907 [Phases 1 and 2 along with sims] (Wave-1+2-Analyses-update.Rmd)
Error in `initializePtr()`:
! function 'cholmod_factor_ldetA' not provided by package 'Matrix'"

This seems to be due to an issue related to the Matrix version (https://stackoverflow.com/questions/77481539/error-in-initializeptr-function-cholmod-factor-ldeta-not-provided-by-pack). I have followed the suggestion and removed the Matrix package and then try to install the binary version. "Warning in install.packages :
  package ‘Matrix’ is not available for this version of R" My version of R is 4.3.2 Unclear which version of R and R packages were used by the authors, so it is not feasible to try and guess it by trial and error (the analyses files load more than 20 packages - unclear how many of these are in fact used).
  I have followed the second suggestion and uninstalled lme4 and then install it again from source. 
  "Quitting from lines 1119-1156 [unnamed-chunk-3] (Wave-1+2-Analyses-update.Rmd)
Error:
! object 'match_fonts' is not exported by 'namespace:systemfonts'"

This is most likely related to using `newggslopegraph` from the package `CGPfunctions`. I commented out the call of this function and tryied to knit again.
  
  "Quitting from lines 1258-1303 [inaccuracy on odd and even month - stability of inaccuracy] (Wave-1+2-Analyses-update.Rmd)
Error in `parse()`:
! <text>:5:71: unexpected '::'" related to `dplyr::dplyr::select`. This version of the file could not have been knitted without error on the device of the authors. I fixed the mistake (two instances in the same code chunk) and tried to knit again. This file has 3233 lines of code. This most recent error was not even half way through. The file makes uses of R packages that are not listed at the start of the file. This means that knitting stops multiple times due to missing packages. Ideally, the list of necessary packages and their version would be shared. The file contains 162 instances where `lmer` was used. Unclear which of these 162 estimated models are reported in the paper. 

"Quitting from lines 1511-1593 [unnamed-chunk-6] (Wave-1+2-Analyses-update.Rmd)
Error in `assert_package()`:
! `quick_docx` requires the "flextable" package. To install, type:
install.packages("flextable")"
Despite following the instruction in the error message and installing the package, the file cannot be knit without error. I have then executed all code before this chunk and then tried to execute the code within the chunk line by line to figure out which line of code generates the error. After executing the code above, the environment has 124 objects. This is only half way through the code. The shere number of objects makes it error prone - easy to make one speling error and then get other results. Commented out the lines of code and tried to knit again.

"Quitting from lines 1692-1856 [SUPPLEMENTARY PHASE 1 analyses] (Wave-1+2-Analyses-update.Rmd)
Error in `lme4::lFormula()`:
! 0 (non-NA) cases"
This error seems to suggest there is a problem with the data used for analysis. After one hour of trying to knit the file, it is unfortunately not feasible to go through the code line by line and try to figure out what the problem is. 


Up to line 1692 where the knitting stopped, This file generates a few csv files that are currently saved in the repo. Checks for these files, if they are reproduced by kniting the file:

- wave1.scores.csv
- wave2.scores.csv
- final.results.csv
- top.t1.csv
- medianMASE.t1.csv
- top.t2.csv
- medianMASE.t2.csv
- contrast1.csv
- contrast2.csv

The first 7 are reproduced. The last 2 are not. 



The README file points to a previous version of `Wave 1+2 Analyses FINAL FOR MANUSCRIPT.Rmd` as the one used to generate the results. The code in this version is different from the most recent one in the repo.


Rmd files are probably not knited by the authors. This guess is based on the fact that multiple Rmd files contain code that generates errors that would not allow knitting on any device. For example, Wave1+2_Merge_2021-07-14.Rmd has duplicate chunk labels ("Duplicate chunk label 'add historic data to data frame'"). Going through all the files, finding and then fixing all errors is not feasible. I estimate that it could take several weeks of full time work, and it is uncertain if in the end the results will be reproduced. There is a very low probability of computaitonal reproducibility, based on how the code and files are structured. There doesn't seem to be a constistent coding style used, which increases the probability of making mistakes and lowers the probability of detecting these mistakes. long lines of code that span multiple lines on screen or that require horizontal scrolling, inconsistent spacing between operators and objects, mix of tidyverse (dplyr) and Rbase functions to process data, partial matching, accessing observations by using manually input row indices rather than filtering on explicing criteria (`# baselines for participant phase 1 submissions
baseline_1 <- dat_hist[nrow(dat_hist) - 12, 1:ncol(dat_hist)]`), code repetition and manual changes rather than using functions, no testing. I think there is a very high probability of mistakes in the code.
  
  



2.  

3.  

4.  check the match between the preregistration plan of the paper (submitted by the authors) and the final paper

5.  check robustness reproducibility by using a different operationalization of forecasting accuracy (i.e., the DV). Instead of MASE, we will use MAPE (mean absolute percent error) and use random effects for domain.

6.  check robustness reproducibility by examining how sensitive the results are when accounting for forecast size (i.e., the number of claims that participating teams self-selected into submitting).

7.  perform a robustness comparison to judgements of learning literature. We will calculate bias and sensitivity of the forecasting for each team by domain combination, by using a regression equation of `actual answer ~ real answer` and then extracting estimated intercept (bias) and slope (sensitivity) for each team and domain. Then, we can estimate an MLM (multilevel model) of `bias ~ 1 + (1|domain)` to determine if bias is different from zero and an MLM of `sensitivity ~ 1 + (1|domain)` to determine if sensitivity is different from zero. If these are different from zero, the forecasts are "biased" and "sensitive". Bias is traditionally .4-.6 on a standardized scale range - any values outside this range would be considered different from traditional results. Sensitivity is traditionally .2 to .4 on a standardized scale range - any values outside this range would be considered different from traditional results.

For items 1 and 2 we will take a more confirmatory approach, as we are trying to reproduce the original results in the paper using the data, code, and materials provided by the authors. For items 3, 4, and 5 we will take a more exploratory approach. We summarize here what we plan to do, but we expect that we will need to make additional choices when performing the analysis that we cannot foresee based on the current knowledge we have about the article and materials. As we start working on this, we might discover aspects that require a change of plan (e.g., due to data availability we cannot estimate the models we plan to). In the final report we will describe our choices.

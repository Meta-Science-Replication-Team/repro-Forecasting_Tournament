"slightly" = 2,
"notatall" = 1,
"4very" = 4,
"5extremely" = 5,
"2slightly" = 2,
"1notatall" = 1,
"3moderately" = 3
) ->
d_temp2$feel
#pivot_wider
d_temp2 %>%
pivot_wider(
id_cols = c(id,vignette),
names_from = option,
names_prefix = "feel_",
values_from = feel
) ->
d_temp2
#merge with d_temp
d_temp %>%
merge(d_temp2, by = c("id", "vignette"), all.x = TRUE) ->
d_temp
#covariates uncoded, so code that
#dplyr::select covariates
d_meitei %>%
dplyr::select(id, contains("oys"), contains("swis"), contains("nfc"), -swis_filter) ->
d_temp3
#pivot longer
d_temp3 %>%
pivot_longer(
cols = -id,
names_to = c("scale", "question"),
names_sep = "_"
) ->
d_temp3
#fortunately all 3 scales use the same responses
#recode
d_temp3$value %>%
recode(
"Strongly Disagree" = 1,
"Somewhat Disagree" = 2,
"Neither agree nor disagree" = 3,
"Somewhat Agree" = 4,
"Strongly Agree" = 5
) %>%
as.numeric() ->
d_temp3$value
#pivot wider
d_temp3 %>%
pivot_wider(
id_cols = id,
names_from = c("scale", "question"),
names_sep = "_"
) ->
d_temp3
#merge with d_temp
d_temp %>%
merge(d_temp3, by = "id", all.x = TRUE) ->
d_temp
#now merge with demographics
d_temp %>%
merge(
d_meitei %>% dplyr::select(
-contains("Marcos"), -contains("Jose"), -contains("Ian"),
-contains("Linda"), -contains("Peter"), -contains("Kathy"),
-contains("oys"), -contains("swis"), -contains("nfc"),
-block, -site, swis_filter
), by = "id"
) ->
d_meitei
#recode demographics
d_meitei$sex %>%
recode(
'1' = "male",
'2' = "female",
'3' = "other"
) ->
d_meitei$sex
d_meitei$philosophy %>%
recode(
'1' = "yes",
'2' = "no"
) ->
d_meitei$philosophy
d_meitei$religion %>%
recode(
'1' = "buddhist",
'2' = "catholic",
'3' = "protestant",
'4' = "mormon",
'5' = "hindu",
'6' = "jewish",
'7' = "muslim",
'8' = "sikh",
'9' = "atheist",
'11' = "christian_orthodox",
'12' = "christian_other",
'13' = "shinto",
'14' = "confucian",
'15' = "daoist",
'16' = "jain",
'17' = "other",
'18' = "atheist"
) ->
d_meitei$religion
rbind(d_canada, d_china, d_german, d_slovak, d_japan, d_korea,
d_mturk, d_pitt, d_morocco, d_ecuador, d_hindi) ->
d
#add south africa, shiwiar using rbind.fill() to fill in NA for absent feel columns
plyr::rbind.fill(d, d_africa, d_shiwiar, d_shipibo, d_tamil, d_meitei) ->
d
rbind(d_canada, d_china, d_german, d_slovak, d_japan, d_korea,
d_mturk, d_pitt, d_morocco, d_ecuador, d_hindi) ->
d
#add south africa, shiwiar using rbind.fill() to fill in NA for absent feel columns
plyr::rbind.fill(d, d_africa, d_shiwiar, d_shipibo, d_tamil, d_meitei) ->
d
d$nfc_3_r<-6 - d$nfc_3
d$nfc_4_r<-6 - d$nfc_4
d$nfc_5_r<-6 - d$nfc_5
d$nfc_7_r<-6 - d$nfc_7
d$nfc_8_r<-6 - d$nfc_8
d$nfc_9_r<-6 - d$nfc_9
d$nfc_12_r<-6 - d$nfc_12
d$nfc_16_r<-6 - d$nfc_16
d$nfc_17_r<-6 - d$nfc_17
NFC<-c("nfc_1","nfc_2","nfc_3_r","nfc_4_r","nfc_5_r","nfc_6","nfc_7_r","nfc_8_r","nfc_9_r","nfc_10","nfc_11","nfc_12_r","nfc_13","nfc_14","nfc_15","nfc_16_r","nfc_17_r","nfc_18") #get NFC column names
#get wide scores for reliability analyses
d_wide_nfc<-d %>%
group_by(id, site) %>%
summarize(across(NFC, ~ mean(.x, na.rm = TRUE)))
psych::describeBy(d_wide_nfc[NFC],group=d_wide_nfc$site)
#get reliability
#total
psych::alpha(d_wide_nfc[NFC]) #reliability is good!
#by country
psych::alpha(subset(d_wide_nfc,site=="canada")[NFC]) #reliability is good!
psych::alpha(subset(d_wide_nfc,site=="china")[NFC]) #reliability is good!
psych::alpha(subset(d_wide_nfc,site=="ecuador")[NFC]) #reliability is good!
fourfactor <- fa(construal.data.short,nfactors = 4,rotate = "oblimin",fm="minres") # TLI = .899 / RMSEA   = .042, 4 cross-loadings, 1 zero loading
#In this section we set Global options for the entire document
#Set the maximum number of rows to be printed in the console to 2,000.
options(max.print=2000)
#Set chunk options as follows: print code = Yes, save cache = Yes,
#Keep right arrow prompt  = No, reformat code = Yes,
#show information messages = No, show warning messages = No.
knitr::opts_chunk$set(echo=T, cache=TRUE, prompt=F, tidy=T, message=FALSE, warning=FALSE)
#Show decimals up to 10 decimal points. After that use e notation.
options(scipen = 3)
#' Removing scientific notation
#options(scipen=999)
library(haven)
library(dplyr)
library(apaTables)
library(jtools)
library(ggplot2)
library(ggthemes) #functions used: theme_gdocs()
library(viridis)
library(sjPlot)
library(ggeffects)
library(ggpubr)
library(psych)
library(GPArotation)
library(tidyr) #functions used: gather()
library(mice)
library(factoextra)
setwd("C:/Users/igrossma/OneDrive - University of Waterloo/data projects/Abstract  Concrete and Wisdom/construal psychometrics")
rawdata<-read.csv('efadata.csv',header=TRUE)
data<-subset(rawdata,Filter==0)
items<-c("abstract_1","abstract_2","abstract_3","abstract_4","abstract_5","abstract_6","abstract_7","abstract_8","abstract_9" ,"abstract_10","abstract_11","abstract_12","abstract_13","abstract_14","abstract_15","abstract_17","abstract_18","abstract_19","abstract_21",  "abstract_22", "abstract_24","abstract_25","abstract_26" , "abstract_27","abstract_28" , "abstract_30","abstract_31" ,"abstract_32","abstract_33", "abstract_34" , "abstract_35" ,"concrete_1","concrete_2","concrete_3","concrete_4","concrete_5","concrete_6","concrete_7","concrete_8","concrete_9","concrete_10","concrete_11","concrete_12",
"ratings_2concrete_13_31","concrete_14","concrete_15","concrete_16","concrete_17" ,"concrete_18","concrete_19","concrete_21", "concrete_22", "concrete_24" ,"concrete_25","concrete_26", "concrete_27","concrete_28","concrete_29" ,"concrete_30" ,"concrete_31","concrete_32","concrete_33","concrete_34")
#create shorter items list, excluding initially generated items for emotionsfeelings and distance, as per recommendation of CLT experts
#Excluded the following abstract
#10.I distanced myself from the experience
#25 I tried to look at myself through a third-person perspective, as an observer would
#and the following concrete
#5.	I thought about how this situation makes me feel
#11.	I felt overwhelmed by emotions
#17.	I focused on how the situation made me feel.
#29.	It evoked strong emotion in me
#30.	I relied on my intuitive impressions of the situation.
#31.	I relied on my instincts when considering ways to handle the situation.
#32.	I used my heart as a guide for my thoughts.
# and remove repeats
#abstract 16 - too similar to abstract 17
#abstract 20.	 same as abstract 1
#abstract 23.	same as abstract 6
#abstract 29 - too similar to abstract7
#concrete 23  - same as concrete 1
items.short<-c("abstract_1","abstract_2","abstract_3","abstract_4","abstract_5","abstract_6","abstract_7","abstract_8","abstract_9" ,"abstract_11","abstract_12","abstract_13","abstract_14","abstract_15","abstract_17","abstract_18","abstract_19","abstract_21",  "abstract_22", "abstract_24","abstract_26" , "abstract_27","abstract_28" , "abstract_30","abstract_31" ,"abstract_32","abstract_33", "abstract_34" , "abstract_35" ,"concrete_1","concrete_2","concrete_3","concrete_4","concrete_6","concrete_7","concrete_8","concrete_9","concrete_10","concrete_12",
"ratings_2concrete_13_31","concrete_14","concrete_15","concrete_16","concrete_18","concrete_19","concrete_21", "concrete_22", "concrete_24" ,"concrete_25","concrete_26", "concrete_27","concrete_28","concrete_33","concrete_34")
construal.data<-data[items]
construal.data.short<-data[items.short]
#First, subset out participants that have too much missing data:
percentmissing = function (x){ sum(is.na(x))/length(x) * 100}
missing = apply(construal.data.short, 1, percentmissing)
table(missing) # 1 misses all, 2 miss a lot, one misses less than 2 percent
replacepeople <- subset(construal.data.short, missing <= 5) # get rid of those who miss more than 5%
apply(replacepeople, 2, percentmissing)
#replace
tempnomiss<- mice(replacepeople)
nomiss<- mice::complete(tempnomiss, 1)
#Create the Mahalanobis values:
mahal <- mahalanobis(construal.data.short,
colMeans(construal.data.short, na.rm = T),
cov(construal.data.short, use = "pairwise.complete.obs"))
#Create the cut off score:
cutoff <- qchisq(1-.001,ncol(construal.data.short))
#Remember you can use:
#cutoff to get the cutoff score
#ncol(dataset) to get the df
#noutliers?
summary(mahal < cutoff)
#31 outliers
noout <- na.omit(subset(nomiss, mahal < cutoff))
correl <- cor(construal.data.short, use = 'pairwise.complete.obs')
#Get the symbols chart:
symnum(correl)
#Look for 1s NOT on the diagonal
#check if enough correlation
cortest.bartlett(correl, n = nrow(construal.data.short))
#Bartlett test is sig, That results implies we have large enough correlations for EFA.
#KMO (Kaiser-Meyer-Olkin) test determines if you have a good sample for EFA. You want high values close to one.
KMO(correl)
#The mean sampling adequacy (MSA) was .94, which is a good score.
parallel <- fa.parallel(noout, fm = 'minres', fa = 'fa')
#suggests between 3 (scree plot) and 5 (parallel matrices) factors; theory suggests two factors, too. so, we start with two and continue until we get a simple structure, kicking out items along the way
#'start with 2 factors (abstract and concrete)
twofactor <- fa(noout,nfactors = 2,rotate = "oblimin",fm="minres")
print(twofactor)
print(twofactor$loadings,cutoff = 0.3) # use. .3 as a cut-off for loadings. TLI = .823 / RMSEA   = .053, 1 cross-loading.
#'move to 3 factors
threefactor <- fa(noout,nfactors = 3,rotate = "oblimin",fm="minres")
print(threefactor$loadings,cutoff = 0.3)  # TLI = .858 / RMSEA   = .047, 5 cross-loadings.
#fit is somewhat better (but not by much and more cross-loadings)
#'move to 5 factors
fivefactor <- fa(noout,nfactors = 5,rotate = "oblimin",fm="minres")
print(fivefactor$loadings,cutoff = 0.3)  # TLI = 904 / RMSEA   = .039
#'move to 3 factors
fourfactor <- fa(noout,nfactors = 4,rotate = "oblimin",fm="minres")
print(fourfactor$loadings,cutoff = 0.3)  # TLI = .858 / RMSEA   = .047, 5 cross-loadings.
fourfactor
print(fivefactor$loadings,cutoff = 0.3)  # TLI = 904 / RMSEA   = .039
print(fourfactor$loadings,cutoff = 0.3)  # TLI = .881 / RMSEA   = .043, 8 cross-loadings.
print(threefactor$loadings,cutoff = 0.3)  # TLI = .858 / RMSEA   = .047, 5 cross-loadings.
print(twofactor$loadings,cutoff = 0.4) # use. .3 as a cut-off for loadings. TLI = .823 / RMSEA   = .053, 1 cross-loading.
print(threefactor$loadings,cutoff = 0.4)  # TLI = .858 / RMSEA   = .047, 5 cross-loadings.
print(threefactor$loadings,cutoff = 0.3)  # TLI = .858 / RMSEA   = .047, 5 cross-loadings.
#'re-run with shorter list
items.short.short<-c("abstract_1","abstract_2","abstract_3","abstract_4","abstract_5","abstract_7","abstract_8","abstract_9" ,"abstract_11","abstract_12","abstract_13","abstract_15","abstract_17","abstract_18","abstract_19","abstract_21", "abstract_24","abstract_26" , "abstract_27","abstract_28" , "abstract_32","abstract_33", "abstract_34" , "abstract_35" ,"concrete_1","concrete_2","concrete_3","concrete_4","concrete_6","concrete_7","concrete_8","concrete_10","concrete_12",
"ratings_2concrete_13_31","concrete_14","concrete_15","concrete_16","concrete_18","concrete_19","concrete_21", "concrete_22", "concrete_24" ,"concrete_25","concrete_26", "concrete_27","concrete_28","concrete_33","concrete_34")
construal.data.short<-noout[items.short.short]
parallel <- fa.parallel(construal.data.short, fm = 'minres', fa = 'fa')
#suggests 3 (screenplot) to 5; test 2 to 4
twofactor <- fa(construal.data.short,nfactors = 2,rotate = "oblimin",fm="minres")
print(twofactor$loadings,cutoff = 0.3) # TLI = .833 / RMSEA   = .054, 4 cross-loadings.
threefactor <- fa(construal.data.short,nfactors = 3,rotate = "oblimin",fm="minres") # TLI = .871 / RMSEA   = .047, no cross-loadings.
print(threefactor$loadings,cutoff = 0.3)
fourfactor <- fa(construal.data.short,nfactors = 4,rotate = "oblimin",fm="minres") # TLI = .899 / RMSEA   = .042, 4 cross-loadings, 1 zero loading
print(fourfactor$loadings,cutoff = 0.3)
fivefactor <- fa(construal.data.short,nfactors = 5,rotate = "oblimin",fm="minres") # TLI = .92 / RMSEA   = .037, 2 cross-loadings, 1 zero loading
print(fivefactor$loadings,cutoff = 0.3) #
fivefactor <- fa(construal.data.short,nfactors = 5,rotate = "oblimin",fm="minres") # TLI = .92 / RMSEA   = .037, 2 cross-loadings, 1 zero loading
fivefactor
print(fivefactor$loadings,cutoff = 0.3) #
print(fourfactor$loadings,cutoff = 0.3)
threefactor <- fa(construal.data.short,nfactors = 3,rotate = "oblimin",fm="minres") # TLI = .871 / RMSEA   = .047, no cross-loadings.
print(threefactor$loadings,cutoff = 0.3)
setwd("C:/Users/igrossma/OneDrive - University of Waterloo/data projects/Abstract  Concrete and Wisdom/construal psychometrics")
rawdata<-read.csv('efadata.csv',header=TRUE)
data<-subset(rawdata,Filter==0)
items<-c("abstract_1","abstract_2","abstract_3","abstract_4","abstract_5","abstract_6","abstract_7","abstract_8","abstract_9" ,"abstract_10","abstract_11","abstract_12","abstract_13","abstract_14","abstract_15","abstract_17","abstract_18","abstract_19","abstract_21",  "abstract_22", "abstract_24","abstract_25","abstract_26" , "abstract_27","abstract_28" , "abstract_30","abstract_31" ,"abstract_32","abstract_33", "abstract_34" , "abstract_35" ,"concrete_1","concrete_2","concrete_3","concrete_4","concrete_5","concrete_6","concrete_7","concrete_8","concrete_9","concrete_10","concrete_11","concrete_12",
"ratings_2concrete_13_31","concrete_14","concrete_15","concrete_16","concrete_17" ,"concrete_18","concrete_19","concrete_21", "concrete_22", "concrete_24" ,"concrete_25","concrete_26", "concrete_27","concrete_28","concrete_29" ,"concrete_30" ,"concrete_31","concrete_32","concrete_33","concrete_34")
#create shorter items list, excluding initially generated items for emotionsfeelings and distance, as per recommendation of CLT experts
#Excluded the following abstract
#10.I distanced myself from the experience
#25 I tried to look at myself through a third-person perspective, as an observer would
#and the following concrete
#5.	I thought about how this situation makes me feel
#11.	I felt overwhelmed by emotions
#17.	I focused on how the situation made me feel.
#29.	It evoked strong emotion in me
#30.	I relied on my intuitive impressions of the situation.
#31.	I relied on my instincts when considering ways to handle the situation.
#32.	I used my heart as a guide for my thoughts.
# and remove repeats
#abstract 16 - too similar to abstract 17
#abstract 20.	 same as abstract 1
#abstract 23.	same as abstract 6
#abstract 29 - too similar to abstract7
#concrete 23  - same as concrete 1
items.short<-c("abstract_1","abstract_2","abstract_3","abstract_4","abstract_5","abstract_6","abstract_7","abstract_8","abstract_9" ,"abstract_11","abstract_12","abstract_13","abstract_14","abstract_15","abstract_17","abstract_18","abstract_19","abstract_21",  "abstract_22", "abstract_24","abstract_26" , "abstract_27","abstract_28" , "abstract_30","abstract_31" ,"abstract_32","abstract_33", "abstract_34" , "abstract_35" ,"concrete_1","concrete_2","concrete_3","concrete_4","concrete_6","concrete_7","concrete_8","concrete_9","concrete_10","concrete_12",
"ratings_2concrete_13_31","concrete_14","concrete_15","concrete_16","concrete_18","concrete_19","concrete_21", "concrete_22", "concrete_24" ,"concrete_25","concrete_26", "concrete_27","concrete_28","concrete_33","concrete_34")
construal.data<-data[items]
construal.data.short<-data[items.short]
#First, subset out participants that have too much missing data:
percentmissing = function (x){ sum(is.na(x))/length(x) * 100}
missing = apply(construal.data, 1, percentmissing)
table(missing) # 1 misses all, 2 miss a lot, one misses less than 2 percent
replacepeople <- subset(construal.data, missing <= 5) # get rid of those who miss more than 5%
apply(replacepeople, 2, percentmissing)
#replace
tempnomiss<- mice(replacepeople)
nomiss<- mice::complete(tempnomiss, 1)
#Create the Mahalanobis values:
mahal <- mahalanobis(construal.data,
colMeans(construal.data, na.rm = T),
cov(construal.data, use = "pairwise.complete.obs"))
#Create the cut off score:
cutoff <- qchisq(1-.001,ncol(construal.data))
#Remember you can use:
#cutoff to get the cutoff score
#ncol(dataset) to get the df
#noutliers?
summary(mahal < cutoff)
#29 outliers
noout <- na.omit(subset(nomiss, mahal < cutoff))
correl <- cor(construal.data, use = 'pairwise.complete.obs')
#Get the symbols chart:
symnum(correl)
#Look for 1s NOT on the diagonal
#check if enough correlation
cortest.bartlett(correl, n = nrow(construal.data))
#Bartlett test is sig, That results implies we have large enough correlations for EFA.
#KMO (Kaiser-Meyer-Olkin) test determines if you have a good sample for EFA. You want high values close to one.
KMO(correl)
#The mean sampling adequacy (MSA) was .94, which is a good score.
require(CCA)
install.packages("CCA")
cc1 <- cc(noout[abstract.items], noout[concrete.items])
abstract.items<-c("abstract_1","abstract_2","abstract_3","abstract_4","abstract_5","abstract_6","abstract_7","abstract_8","abstract_9" ,"abstract_10","abstract_11","abstract_12","abstract_13","abstract_14","abstract_15","abstract_17","abstract_18","abstract_19","abstract_21",  "abstract_22", "abstract_24","abstract_25","abstract_26" , "abstract_27","abstract_28" , "abstract_30","abstract_31" ,"abstract_32","abstract_33", "abstract_34" , "abstract_35")
concrete.items<-c("concrete_1","concrete_2","concrete_3","concrete_4","concrete_5","concrete_6","concrete_7","concrete_8","concrete_9","concrete_10","concrete_11","concrete_12",
"ratings_2concrete_13_31","concrete_14","concrete_15","concrete_16","concrete_17" ,"concrete_18","concrete_19","concrete_21", "concrete_22", "concrete_24" ,"concrete_25","concrete_26", "concrete_27","concrete_28","concrete_29" ,"concrete_30" ,"concrete_31","concrete_32","concrete_33","concrete_34")
require(CCA)
cc1 <- cc(noout[abstract.items], noout[concrete.items])
cc1$cor
abstract<-noout[abstract.items]
concrete<-noout[concrete.items]
cc1 <- cc(abstract, concrete)
cc1$cor
cc1
# raw canonical coefficients
cc1[3:4]
knitr::opts_chunk$set(echo = TRUE)
library(forecast)
library(psych)
library(tidyverse)
library(irr)
library(lme4)
library(ggplot2)
library(tidyr)
library(emmeans)
library(car)
library(jtools)
library(dplyr)
library(ggsci)
library(dplyr)
library(Hmisc)
options(max.print = 20000, scipen = 1000)
setwd("~/GitHub/Forecasting-Tournament") #igor's working directory
dat <- read.csv("dat_for_analyses.csv", stringsAsFactors = FALSE)
dat_long <- read.csv("dat_long.csv", stringsAsFactors = FALSE)
# dataset that only includes academic predictions and those who provided open-ended data
academic_only <- filter(dat, isExpert == 1 & !is.na(Method.code))
#datasets that are filtered by phase (1 = May submission, 2 = November submission)
phase1 <- filter(dat, phase == 1& !is.na(Method.code))
phase2 <- filter(dat, phase == 2& !is.na(Method.code))
# Phase 1 & 2further filtered to only include academics won't be necessary once we have updated objective data
phase1_exp <- filter(phase1, isExpert == 1& !is.na(Method.code))
phase2_exp <-filter(phase2, isExpert == 1& !is.na(Method.code))
pd <- position_dodge(0.7) # move them .07 to the left and right
labels<-c(
eafric = "Exp. African\n-Am. Bias",
easian = "Exp. Asian\n-Am. Bias",
egend = "Exp. \nGender Bias",
iafric = "Imp. African\n-Am. Bias",
iasian = "Imp. Asian\n-Am. Bias",
ideoldem = "Dem.\nSupport",
ideolrep ="Rep.\nSupport",
igend = "Imp.\nGender Bias",
lifesat = "Life\nSatisfaction",
negaffect = "Negative\nAffect",
polar = "Polit.\nPolarization",
posaffect = "Positive\nAffect")
pd <- position_dodge(0.7) # move them .07 to the left and right
#subset long data so that we only examine forecasts/accuracy of most updated scores (among those who decided to update at phase 2), keeping phase 1 predictions of those who did not decide to update.
#Use  revised  - 0 = Only submitted in one phase (initial forecasts for phase 1 / initial forecasts for phase 2), 1 = prediction in both phase 1 & 2
dat_long$Month7<-dat_long$Month-7
dat_long_phase2<-dat_long %>%filter(!(phase == 1 & revised == 1)& !is.na(Method.code)& Method.code!="Ground Truth"& Method.code!="Naive-linear"&Method.code!="Naive-rfw" & Month %in% c(7,8,9,10,11,12))
#get ground truth markers (subset)
objective2<-as.data.frame(filter(dat_long,phase != 1 & !is.na(Method.code)& Method.code=="Ground Truth" & (Month >6|Month<13) ))
#### first, confidence
model.long.phase2.conf<-  lmer(value.dif~domain*confidence*Month7+(1|domain/ResponseId), data=subset(dat_long_phase2, isExpert.factor == 'Academic'), REML=F)
car::Anova(model.long.phase2.conf,type="III") #sig interaction between domain and confidence, also a marginal 3 way - domain x confidence x month
summ(model.long.phase2.conf, digits=4, center=T) #the more confidence, the greater the error!
emtrends(model.long.phase2.conf,specs=pairwise~domain,var="confidence") #confidence does  play a role for explicit asian bias (african american): high confidence more APE, but nothing else
model.long.phase2.team3<-  lmer(value.dif~domain*teamS*Month7+(1|domain/ResponseId), data=dat_long_phase2, REML=F)
car::Anova(model.long.phase2.team3,type="III") #nothing for team size
summ(model.long.phase2.team3, digits=4, center=T) #
emmeans(model.long.phase2.team3, pairwise~teamS|domain, adjust = "none") #explicit asian bias - team more inaccurate than singular
#####apriori defined groups
model.long.phase2.team3<-  lmer(value.dif~domain*teamS*Month7+(1|domain/ResponseId), data=dat_long_phase2, REML=F)
##### second, team type - just academics
dat_long_phase2$teamS<-as.factor(ifelse(dat_long_phase2$team_size.coded>=6,3,ifelse(dat_long_phase2$team_size.coded<6&dat_long_phase2$team_size.coded>1,2,ifelse(dat_long_phase2$team_size.coded==1,1,NA))))
#####apriori defined groups
model.long.phase2.team3<-  lmer(value.dif~domain*teamS*Month7+(1|domain/ResponseId), data=dat_long_phase2, REML=F)
car::Anova(model.long.phase2.team3,type="III") #nothing for team size
summ(model.long.phase2.team3, digits=4, center=T) #
emmeans(model.long.phase2.team3, pairwise~teamS|domain, adjust = "none") #explicit asian bias - team more inaccurate than singular
dat_long_phase2$is_multidisciplinary<-ifelse(dat_long_phase2$discipline=="Multi-disciplinary",1,0)
model.long.phase2.multidis.team<-  lmer(value.dif~domain*is_multidisciplinary*Month7+(1|domain/ResponseId), data=subset(dat_long_phase2, isExpert.factor == 'Academic'), REML=F)
car::Anova(model.long.phase2.multidis.team,type="III") #nothing for interdisciplinary
summ(model.long.phase2.multidis.team, digits=4, center=T)
emmeans(model.long.phase2.multidis.team,pairwise ~is_multidisciplinary|domain, adjust = "none") #for e asian bias, more interdisciplionary is better at M1,and M12,
model.long.phase2.subexpert<-  lmer(value.dif~domain*subexpert*Month7+(1|domain/ResponseId), data=dat_long_phase2, REML=F)
car::Anova(model.long.phase2.subexpert,type="III") #3 way interaction
summ(model.long.phase2.subexpert, digits=4, center=T)
emtrends(model.long.phase2.subexpert,~Month7|domain,var="subexpert", at=list(Month7=c(0,5,11))) #
dat_long_phase2$objectivexpert<-ifelse(dat_long_phase2$pub==1,"Expert",ifelse(dat_long_phase2$pub==2,"Non Expert",NA))
model.long.phase2.obexpert<-  lmer(value.dif~domain*objectivexpert*Month7+(1|domain/ResponseId), data=subset(dat_long_phase2, isExpert.factor == 'Academic'), REML=F)
car::Anova(model.long.phase2.obexpert,type="III") #pubs play a role!
summ(model.long.phase2.obexpert, digits=4, center=T)
emmeans(model.long.phase2.obexpert,pairwise ~objectivexpert|Month7|domain, adjust = "none", at=list(Month7=c(0,5,11))) #nonsig
model.long.phase2.predictors.coded<-  lmer(value.dif~parameters_coded*domain*Month7+(1|domain/ResponseId), data=dat_long_phase2, REML=F)
car::Anova(model.long.phase2.predictors.coded,type="III") #nothing
summ(model.long.phase2.predictors.coded, digits=4, center=T)
emtrends(model.long.phase2.predictors.coded,specs=~domain,var="parameters_coded") #
sjPlot::plot_model(model.long.phase2.predictors.coded,type="int")
model.long.phase2.predictors<-  lmer(value.dif~numpred*domain*Month7+(1|domain/ResponseId), data=dat_long_phase2, REML=F)
car::Anova(model.long.phase2.predictors,type="III") #nothing
summ(model.long.phase2.predictors, digits=4, center=T)
emtrends(model.long.phase2.predictors,specs=~domain,var="numpred") #
#### seventh, complexity of the model
model.long.phase2.complex<-  lmer(value.dif~domain*Method.complex*Month7+(1|domain/ResponseId), data=dat_long_phase2, REML=F)
car::Anova(model.long.phase2.complex,type="III") #complexity matters
summ(model.long.phase2.complex, digits=4, center=T)
emtrends(model.long.phase2.complex,specs=~domain,var="Method.complex") #
sjPlot::plot_model(model.long.phase2.complex,type="int")
model.long.phase2.complex<-  lmer(value.dif~domain*as.factor(Method.complex)*Month7+(1|domain/ResponseId), data=dat_long_phase2, REML=F)
car::Anova(model.long.phase2.complex,type="III") #complexity matters
emmeans(model.long.phase2.complex,pairwise ~as.factor(Method.complex)|Month7|domain, adjust = "none", at=list(Month7=c(0,5,11))) #nonsig
model.phase2.long.wcovid<-  lmer(value.dif~Month7*covidcondyn*domain+(1|domain/ResponseId), data=subset(dat_long_phase2, isExpert.factor == 'Academic'), REML=F) #
car::Anova(model.phase2.long.wcovid,type="III") #significant 3 way interaction, and also main effect
summ(model.phase2.long.wcovid, digits=4, center=T)
emmeans(model.phase2.long.wcovid,pairwise ~covidcondyn|Month7|domain, adjust = "none", at=list(Month7=c(0,5,11))) #nonsig
emmeans(model.phase2.long.wcovid,pairwise ~covidcondyn|Month7|domain, adjust = "none"))
emmeans(model.phase2.long.wcovid,pairwise ~covidcondyn|Month7|domain, adjust = "none")
model.long.phase2.counterfac<-  lmer(value.dif~domain*CounterFactual_Presence_Final*Month7+(1|domain/ResponseId), data=dat_long_phase2, REML=F)
car::Anova(model.long.phase2.counterfac,type="III") #nothing
summ(model.long.phase2.counterfac, digits=4, center=T)
emmeans(model.long.phase2.counterfac,pairwise ~CounterFactual_Presence_Final|domain, adjust = "none") #nonsig
model.long.phase2.counterfac.N<-  lmer(value.dif~domain*counterNum*Month7+(1|domain/ResponseId), data=dat_long_phase2, REML=F)
car::Anova(model.long.phase2.counterfac.N,type="III") #nothing
summ(model.long.phase2.counterfac.N, digits=4, center=T)
emtrends(model.long.phase2.counterfac.N,specs=pairwise~domain,var="counterNum") #
#### Nine and 2/3, number of counterfactuals when reflecting on predictions
model.long.phase2.counterfac.covid<-  lmer(value.dif~domain*as.factor(COVID.Final)*Month7+(1|domain/ResponseId), data=dat_long_phase2, REML=F)
car::Anova(model.long.phase2.counterfac.covid,type="III") #nothing
summ(model.long.phase2.counterfac.covid, digits=4, center=T)
emmeans(model.long.phase2.counterfac.covid,pairwise ~as.factor(COVID.Final)|domain, adjust = "none") #
model.long.phase2.tournexperience<-  lmer(value.dif~domain*as.factor(previous_tournament)*Month7+(1|domain/ResponseId), data=dat_long_phase2, REML=F)
car::Anova(model.long.phase2.tournexperience,type="III") #nothing
summ(model.long.phase2.tournexperience, digits=4, center=T)
emmeans(model.long.phase2.tournexperience,pairwise ~as.factor(previous_tournament)|Month7|domain, adjust = "none", at=list(Month7=c(0,5,11))) #nonsig, except for one
#### Ten: experience with prior previous_tournaments
model.long.phase2.tournexperience<-  lmer(value.dif~domain*as.factor(previous_tournament)*Month7+(1|domain/ResponseId), data=dat_long_phase2, REML=F)
names(dat_long_phase2)
#### Ten: experience with prior previous_tournaments
model.long.phase2.tournexperience<-  lmer(value.dif~domain*as.factor(previous_tournament.coded)*Month7+(1|domain/ResponseId), data=dat_long_phase2, REML=F)
car::Anova(model.long.phase2.tournexperience,type="III") #nothing
summ(model.long.phase2.tournexperience, digits=4, center=T)
emmeans(model.long.phase2.tournexperience,pairwise ~as.factor(previous_tournament)|Month7|domain, adjust = "none", at=list(Month7=c(0,5,11))) #nonsig, except for one
emmeans(model.long.phase2.tournexperience,pairwise ~as.factor(previous_tournament.coded)|Month7|domain, adjust = "none", at=list(Month7=c(0,5,11))) #nonsig, except for one
emmeans(model.long.phase2.tournexperience,pairwise ~as.factor(previous_tournament.coded)|Month7|domain, adjust = "none")
emmeans(model.long.phase2.complex,pairwise ~as.factor(Method.complex)|Month7|domain, adjust = "none", at=list(Month7=c(0,3,5))) #nonsig
model.long.phase2.dataonly<-  lmer(value.dif~domain*as.factor(DataOnly)*Month7+(1|domain/ResponseId), data=dat_long_phase2)
car::Anova(model.long.phase2.dataonly,type="III") #nothing
summ(model.long.phase2.dataonly, digits=4, center=T)
emmeans(model.long.phase2.dataonly,pairwise ~as.factor(DataOnly)|domain, adjust = "none") #nonsig
null.model.phase2.academ<-  lmer(MASE1_w2~(1|ResponseId), data=academic_only)
summ(null.model.phase2.academ, digits=4)
#phase 1
null.model.phase1.academ<-  lmer(MASE1_w1~(1|ResponseId), data=academic_only)
summ(null.model.phase1.academ, digits=4) #icc is .7912
null.model.phase1.lay<-  lmer(MASE1_w1~(1|ResponseId), data=select(phase1,Method.code=="Lay People"))
null.model.phase1.lay<-  lmer(MASE1_w1~(1|ResponseId), data=select(phase1,Method.coded=="Lay People"))
phase1$Method.code
null.model.phase1.lay<-  lmer(MASE1_w1~(1|ResponseId), data=subset(phase1,Method.coded=="Lay People"))
null.model.phase1.lay<-  lmer(MASE1_w1~(1|ResponseId), data=subset(phase1,Method.coded=='Lay People'))
null.model.phase1.lay<-  lmer(MASE1_w1~(1|ResponseId), data=filter(phase1, isExpert == 0))
summ(null.model.phase1.lay, digits=4) #icc is .9198
#phase 1
null.model.phase1.academ<-  lmer(MASE1_w1~(1|ResponseId), data=academic_only)
summ(null.model.phase1.academ, digits=4) #icc is .9198
table(academic_only$ResponseId)
#academics
null.model.phase2.academ<-  lmer(MASE1_w2~(1|ResponseId), data=filter(dat_phase2, isExpert == 1))
dat_phase2<-academic_only %>%filter(!(phase == 1 & revised == 1)& !is.na(Method.code)) #just academics
#academics
null.model.phase2.academ<-  lmer(MASE1_w2~(1|ResponseId), data=dat_phase2)
summ(null.model.phase2.academ, digits=4) #icc is .7912
table(dat_phase2$ResponseId)
#phase 1
null.model.phase1.academ<-  lmer(MASE1_w1~(1|ResponseId), data=filter(phase1, isExpert == 1))
summ(null.model.phase1.academ, digits=4) #icc is .9198
table(filter(phase1, isExpert == 1)$ResponseId)
table(dat_phase2$ResponseId)
null.model.phase1.lay<-  lmer(MASE1_w1~(1|ResponseId), data=filter(phase1, isExpert == 0))
summ(null.model.phase1.lay, digits=4) #icc is .703
#academics
null.model.phase2.academ<-  lmer(MASE1_w2~(1|ResponseId), data=dat_phase2)
summ(null.model.phase2.academ, digits=4) #icc is 86

library(ggplot2)
library(ggthemes) #functions used: theme_gdocs()
library(viridis)
library(sjPlot)
library(ggeffects)
library(ggpubr)
library(psych)
library(GPArotation)
library(tidyr) #functions used: gather()
library(mice)
library(factoextra)
setwd("C:/Users/igrossma/OneDrive - University of Waterloo/data projects/Abstract  Concrete and Wisdom/construal psychometrics")
rawdata<-read.csv('efadata.csv',header=TRUE)
data<-subset(rawdata,Filter==0)
items<-c("abstract_1","abstract_2","abstract_3","abstract_4","abstract_5","abstract_6","abstract_7","abstract_8","abstract_9" ,"abstract_10","abstract_11","abstract_12","abstract_13","abstract_14","abstract_15","abstract_17","abstract_18","abstract_19","abstract_21",  "abstract_22", "abstract_24","abstract_25","abstract_26" , "abstract_27","abstract_28" , "abstract_30","abstract_31" ,"abstract_32","abstract_33", "abstract_34" , "abstract_35" ,"concrete_1","concrete_2","concrete_3","concrete_4","concrete_5","concrete_6","concrete_7","concrete_8","concrete_9","concrete_10","concrete_11","concrete_12",
"ratings_2concrete_13_31","concrete_14","concrete_15","concrete_16","concrete_17" ,"concrete_18","concrete_19","concrete_21", "concrete_22", "concrete_24" ,"concrete_25","concrete_26", "concrete_27","concrete_28","concrete_29" ,"concrete_30" ,"concrete_31","concrete_32","concrete_33","concrete_34")
#create shorter items list, excluding initially generated items for emotionsfeelings and distance, as per recommendation of CLT experts
#Excluded the following abstract
#10.I distanced myself from the experience
#25 I tried to look at myself through a third-person perspective, as an observer would
#and the following concrete
#5.	I thought about how this situation makes me feel
#11.	I felt overwhelmed by emotions
#17.	I focused on how the situation made me feel.
#29.	It evoked strong emotion in me
#30.	I relied on my intuitive impressions of the situation.
#31.	I relied on my instincts when considering ways to handle the situation.
#32.	I used my heart as a guide for my thoughts.
# and remove repeats
#abstract 16 - too similar to abstract 17
#abstract 20.	 same as abstract 1
#abstract 23.	same as abstract 6
#abstract 29 - too similar to abstract7
#concrete 23  - same as concrete 1
items.short<-c("abstract_1","abstract_2","abstract_3","abstract_4","abstract_5","abstract_6","abstract_7","abstract_8","abstract_9" ,"abstract_11","abstract_12","abstract_13","abstract_14","abstract_15","abstract_17","abstract_18","abstract_19","abstract_21",  "abstract_22", "abstract_24","abstract_26" , "abstract_27","abstract_28" , "abstract_30","abstract_31" ,"abstract_32","abstract_33", "abstract_34" , "abstract_35" ,"concrete_1","concrete_2","concrete_3","concrete_4","concrete_6","concrete_7","concrete_8","concrete_9","concrete_10","concrete_12",
"ratings_2concrete_13_31","concrete_14","concrete_15","concrete_16","concrete_18","concrete_19","concrete_21", "concrete_22", "concrete_24" ,"concrete_25","concrete_26", "concrete_27","concrete_28","concrete_33","concrete_34")
construal.data<-data[items]
construal.data.short<-data[items.short]
#First, subset out participants that have too much missing data:
percentmissing = function (x){ sum(is.na(x))/length(x) * 100}
missing = apply(construal.data.short, 1, percentmissing)
table(missing) # 1 misses all, 2 miss a lot, one misses less than 2 percent
replacepeople <- subset(construal.data.short, missing <= 5) # get rid of those who miss more than 5%
apply(replacepeople, 2, percentmissing)
#replace
tempnomiss<- mice(replacepeople)
nomiss<- mice::complete(tempnomiss, 1)
#Create the Mahalanobis values:
mahal <- mahalanobis(construal.data.short,
colMeans(construal.data.short, na.rm = T),
cov(construal.data.short, use = "pairwise.complete.obs"))
#Create the cut off score:
cutoff <- qchisq(1-.001,ncol(construal.data.short))
#Remember you can use:
#cutoff to get the cutoff score
#ncol(dataset) to get the df
#noutliers?
summary(mahal < cutoff)
#31 outliers
noout <- na.omit(subset(nomiss, mahal < cutoff))
correl <- cor(construal.data.short, use = 'pairwise.complete.obs')
#Get the symbols chart:
symnum(correl)
#Look for 1s NOT on the diagonal
#check if enough correlation
cortest.bartlett(correl, n = nrow(construal.data.short))
#Bartlett test is sig, That results implies we have large enough correlations for EFA.
#KMO (Kaiser-Meyer-Olkin) test determines if you have a good sample for EFA. You want high values close to one.
KMO(correl)
#The mean sampling adequacy (MSA) was .94, which is a good score.
parallel <- fa.parallel(noout, fm = 'minres', fa = 'fa')
#suggests between 3 (scree plot) and 5 (parallel matrices) factors; theory suggests two factors, too. so, we start with two and continue until we get a simple structure, kicking out items along the way
#'start with 2 factors (abstract and concrete)
twofactor <- fa(noout,nfactors = 2,rotate = "oblimin",fm="minres")
print(twofactor)
print(twofactor$loadings,cutoff = 0.3) # use. .3 as a cut-off for loadings. TLI = .823 / RMSEA   = .053, 1 cross-loading.
#'move to 3 factors
threefactor <- fa(noout,nfactors = 3,rotate = "oblimin",fm="minres")
print(threefactor$loadings,cutoff = 0.3)  # TLI = .858 / RMSEA   = .047, 5 cross-loadings.
#fit is somewhat better (but not by much and more cross-loadings)
#'move to 5 factors
fivefactor <- fa(noout,nfactors = 5,rotate = "oblimin",fm="minres")
print(fivefactor$loadings,cutoff = 0.3)  # TLI = 904 / RMSEA   = .039
#'move to 3 factors
fourfactor <- fa(noout,nfactors = 4,rotate = "oblimin",fm="minres")
print(fourfactor$loadings,cutoff = 0.3)  # TLI = .858 / RMSEA   = .047, 5 cross-loadings.
fourfactor
print(fivefactor$loadings,cutoff = 0.3)  # TLI = 904 / RMSEA   = .039
print(fourfactor$loadings,cutoff = 0.3)  # TLI = .881 / RMSEA   = .043, 8 cross-loadings.
print(threefactor$loadings,cutoff = 0.3)  # TLI = .858 / RMSEA   = .047, 5 cross-loadings.
print(twofactor$loadings,cutoff = 0.4) # use. .3 as a cut-off for loadings. TLI = .823 / RMSEA   = .053, 1 cross-loading.
print(threefactor$loadings,cutoff = 0.4)  # TLI = .858 / RMSEA   = .047, 5 cross-loadings.
print(threefactor$loadings,cutoff = 0.3)  # TLI = .858 / RMSEA   = .047, 5 cross-loadings.
#'re-run with shorter list
items.short.short<-c("abstract_1","abstract_2","abstract_3","abstract_4","abstract_5","abstract_7","abstract_8","abstract_9" ,"abstract_11","abstract_12","abstract_13","abstract_15","abstract_17","abstract_18","abstract_19","abstract_21", "abstract_24","abstract_26" , "abstract_27","abstract_28" , "abstract_32","abstract_33", "abstract_34" , "abstract_35" ,"concrete_1","concrete_2","concrete_3","concrete_4","concrete_6","concrete_7","concrete_8","concrete_10","concrete_12",
"ratings_2concrete_13_31","concrete_14","concrete_15","concrete_16","concrete_18","concrete_19","concrete_21", "concrete_22", "concrete_24" ,"concrete_25","concrete_26", "concrete_27","concrete_28","concrete_33","concrete_34")
construal.data.short<-noout[items.short.short]
parallel <- fa.parallel(construal.data.short, fm = 'minres', fa = 'fa')
#suggests 3 (screenplot) to 5; test 2 to 4
twofactor <- fa(construal.data.short,nfactors = 2,rotate = "oblimin",fm="minres")
print(twofactor$loadings,cutoff = 0.3) # TLI = .833 / RMSEA   = .054, 4 cross-loadings.
threefactor <- fa(construal.data.short,nfactors = 3,rotate = "oblimin",fm="minres") # TLI = .871 / RMSEA   = .047, no cross-loadings.
print(threefactor$loadings,cutoff = 0.3)
fourfactor <- fa(construal.data.short,nfactors = 4,rotate = "oblimin",fm="minres") # TLI = .899 / RMSEA   = .042, 4 cross-loadings, 1 zero loading
print(fourfactor$loadings,cutoff = 0.3)
fivefactor <- fa(construal.data.short,nfactors = 5,rotate = "oblimin",fm="minres") # TLI = .92 / RMSEA   = .037, 2 cross-loadings, 1 zero loading
print(fivefactor$loadings,cutoff = 0.3) #
fivefactor <- fa(construal.data.short,nfactors = 5,rotate = "oblimin",fm="minres") # TLI = .92 / RMSEA   = .037, 2 cross-loadings, 1 zero loading
fivefactor
print(fivefactor$loadings,cutoff = 0.3) #
print(fourfactor$loadings,cutoff = 0.3)
threefactor <- fa(construal.data.short,nfactors = 3,rotate = "oblimin",fm="minres") # TLI = .871 / RMSEA   = .047, no cross-loadings.
print(threefactor$loadings,cutoff = 0.3)
setwd("C:/Users/igrossma/OneDrive - University of Waterloo/data projects/Abstract  Concrete and Wisdom/construal psychometrics")
rawdata<-read.csv('efadata.csv',header=TRUE)
data<-subset(rawdata,Filter==0)
items<-c("abstract_1","abstract_2","abstract_3","abstract_4","abstract_5","abstract_6","abstract_7","abstract_8","abstract_9" ,"abstract_10","abstract_11","abstract_12","abstract_13","abstract_14","abstract_15","abstract_17","abstract_18","abstract_19","abstract_21",  "abstract_22", "abstract_24","abstract_25","abstract_26" , "abstract_27","abstract_28" , "abstract_30","abstract_31" ,"abstract_32","abstract_33", "abstract_34" , "abstract_35" ,"concrete_1","concrete_2","concrete_3","concrete_4","concrete_5","concrete_6","concrete_7","concrete_8","concrete_9","concrete_10","concrete_11","concrete_12",
"ratings_2concrete_13_31","concrete_14","concrete_15","concrete_16","concrete_17" ,"concrete_18","concrete_19","concrete_21", "concrete_22", "concrete_24" ,"concrete_25","concrete_26", "concrete_27","concrete_28","concrete_29" ,"concrete_30" ,"concrete_31","concrete_32","concrete_33","concrete_34")
#create shorter items list, excluding initially generated items for emotionsfeelings and distance, as per recommendation of CLT experts
#Excluded the following abstract
#10.I distanced myself from the experience
#25 I tried to look at myself through a third-person perspective, as an observer would
#and the following concrete
#5.	I thought about how this situation makes me feel
#11.	I felt overwhelmed by emotions
#17.	I focused on how the situation made me feel.
#29.	It evoked strong emotion in me
#30.	I relied on my intuitive impressions of the situation.
#31.	I relied on my instincts when considering ways to handle the situation.
#32.	I used my heart as a guide for my thoughts.
# and remove repeats
#abstract 16 - too similar to abstract 17
#abstract 20.	 same as abstract 1
#abstract 23.	same as abstract 6
#abstract 29 - too similar to abstract7
#concrete 23  - same as concrete 1
items.short<-c("abstract_1","abstract_2","abstract_3","abstract_4","abstract_5","abstract_6","abstract_7","abstract_8","abstract_9" ,"abstract_11","abstract_12","abstract_13","abstract_14","abstract_15","abstract_17","abstract_18","abstract_19","abstract_21",  "abstract_22", "abstract_24","abstract_26" , "abstract_27","abstract_28" , "abstract_30","abstract_31" ,"abstract_32","abstract_33", "abstract_34" , "abstract_35" ,"concrete_1","concrete_2","concrete_3","concrete_4","concrete_6","concrete_7","concrete_8","concrete_9","concrete_10","concrete_12",
"ratings_2concrete_13_31","concrete_14","concrete_15","concrete_16","concrete_18","concrete_19","concrete_21", "concrete_22", "concrete_24" ,"concrete_25","concrete_26", "concrete_27","concrete_28","concrete_33","concrete_34")
construal.data<-data[items]
construal.data.short<-data[items.short]
#First, subset out participants that have too much missing data:
percentmissing = function (x){ sum(is.na(x))/length(x) * 100}
missing = apply(construal.data, 1, percentmissing)
table(missing) # 1 misses all, 2 miss a lot, one misses less than 2 percent
replacepeople <- subset(construal.data, missing <= 5) # get rid of those who miss more than 5%
apply(replacepeople, 2, percentmissing)
#replace
tempnomiss<- mice(replacepeople)
nomiss<- mice::complete(tempnomiss, 1)
#Create the Mahalanobis values:
mahal <- mahalanobis(construal.data,
colMeans(construal.data, na.rm = T),
cov(construal.data, use = "pairwise.complete.obs"))
#Create the cut off score:
cutoff <- qchisq(1-.001,ncol(construal.data))
#Remember you can use:
#cutoff to get the cutoff score
#ncol(dataset) to get the df
#noutliers?
summary(mahal < cutoff)
#29 outliers
noout <- na.omit(subset(nomiss, mahal < cutoff))
correl <- cor(construal.data, use = 'pairwise.complete.obs')
#Get the symbols chart:
symnum(correl)
#Look for 1s NOT on the diagonal
#check if enough correlation
cortest.bartlett(correl, n = nrow(construal.data))
#Bartlett test is sig, That results implies we have large enough correlations for EFA.
#KMO (Kaiser-Meyer-Olkin) test determines if you have a good sample for EFA. You want high values close to one.
KMO(correl)
#The mean sampling adequacy (MSA) was .94, which is a good score.
require(CCA)
install.packages("CCA")
cc1 <- cc(noout[abstract.items], noout[concrete.items])
abstract.items<-c("abstract_1","abstract_2","abstract_3","abstract_4","abstract_5","abstract_6","abstract_7","abstract_8","abstract_9" ,"abstract_10","abstract_11","abstract_12","abstract_13","abstract_14","abstract_15","abstract_17","abstract_18","abstract_19","abstract_21",  "abstract_22", "abstract_24","abstract_25","abstract_26" , "abstract_27","abstract_28" , "abstract_30","abstract_31" ,"abstract_32","abstract_33", "abstract_34" , "abstract_35")
concrete.items<-c("concrete_1","concrete_2","concrete_3","concrete_4","concrete_5","concrete_6","concrete_7","concrete_8","concrete_9","concrete_10","concrete_11","concrete_12",
"ratings_2concrete_13_31","concrete_14","concrete_15","concrete_16","concrete_17" ,"concrete_18","concrete_19","concrete_21", "concrete_22", "concrete_24" ,"concrete_25","concrete_26", "concrete_27","concrete_28","concrete_29" ,"concrete_30" ,"concrete_31","concrete_32","concrete_33","concrete_34")
require(CCA)
cc1 <- cc(noout[abstract.items], noout[concrete.items])
cc1$cor
abstract<-noout[abstract.items]
concrete<-noout[concrete.items]
cc1 <- cc(abstract, concrete)
cc1$cor
cc1
# raw canonical coefficients
cc1[3:4]
knitr::opts_chunk$set(echo = TRUE)
library(forecast)
library(psych)
library(tidyverse)
library(irr)
library(lme4)
library(ggplot2)
library(tidyr)
library(emmeans)
library(car)
library(jtools)
library(dplyr)
library(ggsci)
library(dplyr)
library(Hmisc)
options(max.print = 20000, scipen = 1000)
setwd("~/GitHub/Forecasting-Tournament") #igor's working directory
dat <- read.csv("dat_for_analyses.csv", stringsAsFactors = FALSE)
dat_long <- read.csv("dat_long.csv", stringsAsFactors = FALSE)
# dataset that only includes academic predictions and those who provided open-ended data
academic_only <- filter(dat, isExpert == 1 )
#datasets that are filtered by phase (1 = May submission, 2 = November submission)
phase1 <- filter(dat, phase == 1)
phase2 <- filter(dat, phase == 2)
# Phase 1 & 2further filtered to only include academics won't be necessary once we have updated objective data
phase1_exp <- filter(phase1, isExpert == 1)
phase2_exp <-filter(phase2, isExpert == 1)
#####download of phase 1 and 2 files########################
t1.academ.sorted<-phase1_exp %>%
arrange(domain,MASE1_w1) %>%group_by(domain) %>% select(team_name,domain,Month.1:Month.12,mean_abs_error_w1,MASE1_w1,Method.code)
t1.nonacadem.av.sorted<-phase1 %>% filter(isExpert.factor == 'Prolific') %>% select(team_name,domain,Month.1:Month.12,mean_abs_error_w1,MASE1_w1,Method.code) %>%
group_by(domain) %>% summarise(across(where(is.numeric), mean)) %>% arrange(domain,MASE1_w1) %>% mutate(team_name="average non-academic")
t1.nonacadem.median.sorted<-phase1 %>% filter(isExpert.factor == 'Prolific') %>% select(team_name,domain,Month.1:Month.12,mean_abs_error_w1,MASE1_w1,,Method.code) %>%
group_by(domain) %>% summarise(across(where(is.numeric), median)) %>% arrange(domain,MASE1_w1) %>% mutate(team_name="median non-academic")
t1.nonacadem.best.sorted<-phase1 %>% filter(isExpert.factor == 'Prolific') %>% select(team_name,domain,Month.1:Month.12,mean_abs_error_w1,MASE1_w1,Method.code) %>%
group_by(domain) %>% summarise(across(where(is.numeric), min)) %>% arrange(domain,MASE1_w1) %>% mutate(team_name="top non-academic")
t1.academ.best.sorted<-phase1 %>% filter(isExpert.factor == 'Academic') %>% select(team_name,domain,Month.1:Month.12,mean_abs_error_w1,MASE1_w1,Method.code) %>%
group_by(domain) %>% summarise(across(where(is.numeric), min)) %>% arrange(domain,MASE1_w1) %>% mutate(team_name="top academic")
t1.top.scores<-rbind(t1.academ.best.sorted,t1.nonacadem.best.sorted)%>% arrange(domain,MASE1_w1)
#so, only for life satisfaction and polarization, best academic was better than best non-academic. For all other domains, non-academics were in fact better (but note that the sample of non-academic was larger)
#what is the percentage of academics and lay people, respectively, who were below 1 on MASE?
t1.scores<-rbind(t1.academ.sorted,t1.nonacadem.median.sorted)
write.csv(t1.scores,"wave1.scores.csv")
t2.academ.sorted<-academic_only %>%
arrange(domain,MASE1_w2) %>%group_by(domain) %>% select(team_name,domain,Month.7:Month.12,mean_abs_error_w2,MASE1_w2,Method.code,phase,revised)
write.csv(t2.academ.sorted,"wave2.scores.csv")
ormers}
pd <- position_dodge(0.7) # move them .07 to the left and right
labels<-c(
eafric = "Exp. African\n-Am. Bias",
easian = "Exp. Asian\n-Am. Bias",
egend = "Exp. \nGender Bias",
iafric = "Imp. African\n-Am. Bias",
iasian = "Imp. Asian\n-Am. Bias",
ideoldem = "Dem.\nSupport",
ideolrep ="Rep.\nSupport",
igend = "Imp.\nGender Bias",
lifesat = "Life\nSatisfaction",
negaffect = "Negative\nAffect",
polar = "Polit.\nPolarization",
posaffect = "Positive\nAffect")
#do by method (among experts now)
#get ground truth markers (subset)
dat_long$Month0<-dat_long$Month-1
objective<-as.data.frame(subset(dat_long,phase == 1 & !is.na(Method.code)& Method.code=="Ground Truth"))
dat_long_phase1<-dat_long %>%subset(phase == 1 & Method.code!="Ground Truth"& Method.code!="Naive-linear"&Method.code!="Naive-rfw")
dat_long_phase1$Method.code <- relevel(factor(dat_long_phase1$Method.code), "Lay People") #use lay people as a reference group
phase1$Method.code <- relevel(factor(phase1$Method.code), "Lay People") #use lay people as a reference group
#examine vs. benchmarks of accuracy
## naive random walk
#to be able to calculate estimate scores==> equate at naive RW to below.
phase1$compare_to_naive_rwf_MASE.update<-ifelse(phase1$compare_to_naive_rwf_MASE!="Equal to Naive rwf",phase1$compare_to_naive_rwf_MASE,ifelse(phase1$compare_to_naive_rwf_MASE=="Equal to Naive rwf","Below Naive rwf",NA))
model.phase1.belowrw<-  lmer(MASE1_w1~domain*Method.code+(1|ResponseId), data=subset(phase1,compare_to_naive_rwf_MASE.update=="Below Naive rwf"))
model.phase1.aboverw<-  lmer(MASE1_w1~domain*Method.code+(1|ResponseId), data=subset(phase1,compare_to_naive_rwf_MASE.update=="Above Naive rwf"))
data.phase1.MASE.belowrw<-as.data.frame(emmeans(model.phase1.belowrw,pairwise ~Method.code*domain, adjust = "none")$emmeans)
data.phase1.MASE.aboverw<-as.data.frame(emmeans(model.phase1.aboverw,pairwise ~Method.code*domain, adjust = "none")$emmeans)
data.phase1.MASE.belowrw$cut<-"Better than Random Walk"
data.phase1.MASE.aboverw$cut<-"Worse than Random Walk"
data.phase1.MASE.rw<-rbind(data.phase1.MASE.belowrw,data.phase1.MASE.aboverw)
?facet_wrap
data.phase1.MASE.rw %>%
ggplot(aes(x = domain, y = emmean, colour = Method.code,fill=Method.code))+
geom_pointrange(aes(ymin=lower.CL, ymax=upper.CL), position=pd)+  theme_minimal(base_size = 14) +geom_hline(yintercept =1, linetype='dashed', color='red', 14)+facet_wrap(~cut, nrow=2, scale="free")+
theme(legend.position="bottom") +scale_color_d3()+scale_fill_d3()+  scale_x_discrete(labels=labels, name="")+
labs(colour = "",fill="", x="",y="MASE (M +/- 95%CI)")
data.phase1.MASE.rw %>%
ggplot(aes(x = domain, y = emmean, colour = Method.code,fill=Method.code))+
geom_pointrange(aes(ymin=lower.CL, ymax=upper.CL), position=pd)+  theme_minimal(base_size = 14) +geom_hline(yintercept =1, linetype='dashed', color='red', 14)+facet_wrap(~cut, nrow=2, scale="free")+
theme(legend.position="bottom") +scale_color_d3()+scale_fill_d3()+  scale_x_discrete(labels=labels, name="")+expand_limits(y=0)+
labs(colour = "",fill="", x="",y="MASE (M +/- 95%CI)")
data.phase1.MASE.rw %>%
ggplot(aes(x = domain, y = emmean, colour = Method.code,fill=Method.code))+
geom_pointrange(aes(ymin=lower.CL, ymax=upper.CL), position=pd)+  theme_minimal(base_size = 14) +geom_hline(yintercept =1, linetype='dashed', color='red', 14)+facet_wrap(~cut, nrow=2, scale="free")+
theme(legend.position="bottom") +scale_color_d3()+scale_fill_d3()+  scale_x_discrete(labels=labels, name="")+scale_y_continuous(limits=c(0,NA))+
labs(colour = "",fill="", x="",y="MASE (M +/- 95%CI)")
data.phase1.MASE.rw %>%
ggplot(aes(x = domain, y = emmean, colour = Method.code,fill=Method.code))+
geom_pointrange(aes(ymin=lower.CL, ymax=upper.CL), position=pd)+  theme_minimal(base_size = 14) +geom_hline(yintercept =1, linetype='dashed', color='red', 14)+facet_wrap(~cut, nrow=2, scale="free")+
theme(legend.position="bottom") +scale_color_d3()+scale_fill_d3()+  scale_x_discrete(labels=labels, name="")+
labs(colour = "",fill="", x="",y="MASE (M +/- 95%CI)")
emmeans(model.phase1.belowrw,trt.vs.ctrl ~Method.code|domain, adjust = "none") #lay vs. rest
emmeans(model.phase1.aboverw,trt.vs.ctrl ~Method.code|domain, adjust = "none") #lay vs. rest
phase1$compare_to_naive_linear_MASE.update<-ifelse(phase1$compare_to_naive_linear_MASE!="Equal to Naive linear",phase1$compare_to_naive_linear_MASE,ifelse(phase1$compare_to_naive_linear_MASE=="Equal to Naive linear","Below Naive linear",NA))
model.phase1.belowlinear<-  lmer(MASE1_w1~domain*Method.code+(1|ResponseId), data=subset(phase1,compare_to_naive_linear_MASE.update=="Below Naive linear"))
model.phase1.abovelinear<-  lmer(MASE1_w1~domain*Method.code+(1|ResponseId), data=subset(phase1,compare_to_naive_linear_MASE.update=="Above Naive linear"))
data.phase1.MASE.belowlinear<-as.data.frame(emmeans(model.phase1.belowlinear,pairwise ~Method.code*domain, adjust = "none")$emmeans)
data.phase1.MASE.abovelinear<-as.data.frame(emmeans(model.phase1.abovelinear,pairwise ~Method.code*domain, adjust = "none")$emmeans)
data.phase1.MASE.belowlinear$cut<-"Better than Linear Regression"
data.phase1.MASE.abovelinear$cut<-"Worse than Linear Regression"
data.phase1.MASE.linear<-rbind(data.phase1.MASE.belowlinear,data.phase1.MASE.abovelinear)
data.phase1.MASE.linear %>%
ggplot(aes(x = domain, y = emmean, colour = Method.code,fill=Method.code))+
geom_pointrange(aes(ymin=lower.CL, ymax=upper.CL), position=pd)+  theme_minimal(base_size = 14) +geom_hline(yintercept =1, linetype='dashed', color='red', 14)+facet_wrap(~cut, nrow=2, scale="free")+
theme(legend.position="bottom") +scale_color_d3()+scale_fill_d3()+  scale_x_discrete(labels=labels, name="")+
labs(colour = "",fill="", x="",y="MASE (M +/- 95%CI)")
emmeans(model.phase1.belowlinear,trt.vs.ctrl ~Method.code|domain, adjust = "none") #lay vs. rest
emmeans(model.phase1.abovelinear,trt.vs.ctrl ~Method.code|domain, adjust = "none") #lay vs. rest
# What percentage is better than naive linear regression benchmark, per domain
naive_RMSE_domain <- phase1_exp %>% group_by(domain, RMSE_cutoff_Naive_linear.factor) %>%
dplyr::summarise(N = length(RMSE_cutoff_Naive_linear.factor)) %>% ungroup() %>%
group_by(domain) %>% mutate(ptg = prop.table(N)*100) %>% ungroup() %>%
arrange(by_group=RMSE_cutoff_Naive_linear.factor,desc(ptg))
knitr::kable((naive_RMSE_domain))
naive_linear_MASE_domain <- phase1_exp %>% group_by(domain, MASE_cutoff_Naive_linear) %>%
dplyr::summarise(N = length(MASE_cutoff_Naive_linear)) %>% ungroup() %>%
group_by(domain) %>% mutate(ptg = prop.table(N)*100) %>% ungroup() %>%
arrange(by_group=MASE_cutoff_Naive_linear,desc(ptg))
knitr::kable((naive_linear_MASE_domain))
View(phase1_exp$MASE_cutoff_Naive_linear,phase1_exp$compare_to_naive_rwf_MASE)
View(phase1_exp$compare_to_naive_rwf_MASE)
phase1_exp$compare_to_naive_linear
phase1_exp$compare_to_naive_linear_MASE
# What percentage is better than naive linear regression benchmark, per domain
naive_linear_MASE_domain <- phase1_exp %>% group_by(domain, compare_to_naive_linear_MASE) %>%
dplyr::summarise(N = length(compare_to_naive_linear_MASE)) %>% ungroup() %>%
group_by(domain) %>% mutate(ptg = prop.table(N)*100) %>% ungroup() %>%
arrange(by_group=compare_to_naive_linear_MASE,desc(ptg))
knitr::kable((naive_linear_MASE_domain))
# What percentage is better than naive random walk benchmark, per domain
naive_rwf_MASE_domain <- phase1_exp %>% group_by(domain, compare_to_naive_rwf_MASE) %>%
dplyr::summarise(N = length(compare_to_naive_rwf_MASE)) %>% ungroup() %>%
group_by(domain) %>% mutate(ptg = prop.table(N)*100) %>% ungroup() %>%
arrange(by_group=compare_to_naive_rwf_MASE,desc(ptg))
knitr::kable((naive_rwf_MASE_domain))
phase1.means<-as.data.frame(emmeans(model.phase1,pairwise ~domain|Method.code, adjust = "none")$emmeans)
# reorder and get the order variable to maintain facet-specific orders (otherwise it get screwy)
phase1.means <- phase1.means %>%
# 1. Remove grouping
ungroup() %>%
# 2. Arrange by
#   i.  facet group
#   ii. bar height
arrange(Method.code, emmean) %>%
# 3. Add order column of row numbers
mutate(order = row_number())
#lollipop chart
phase1.means %>% ggplot(aes(x=order, y=emmean, color=Method.code)) +
geom_point(size=3) +
geom_segment(aes(x=order,
xend=order,
y=0,
yend=emmean)) +facet_wrap(vars(Method.code), scales = "free")+
scale_x_continuous(   # Add categories to axis
breaks = phase1.means$order,
labels = phase1.means$domain)+theme_minimal(base_size = 14) +geom_hline(yintercept =1, linetype='dashed', color='red', 14)+theme(legend.position="none")+
scale_color_aaas()+ labs(colour = "Method",x="",y="Average MASE")
#errorbar charts, with scores ordered
phase1.means %>% ggplot(aes(x=order, y=emmean, color=Method.code)) +
geom_point(size=3) +
geom_errorbar(aes(ymin=lower.CL, ymax=upper.CL))+
facet_wrap(vars(Method.code), scales = "free", nrow=4)+theme_minimal() +geom_hline(yintercept =1, linetype='dashed', color='red', 14)+theme(legend.position="none")+
scale_x_continuous(   # Add categories to axis
breaks = phase1.means$order,
labels = phase1.means$domain,expand = c(0,0))+
scale_color_aaas()+ labs(colour = "Method",x="",y="Average MASE +/- 95% CI")
#get scores for ranking visualizations
phase1.means<-as.data.frame(emmeans(model.phase1,pairwise ~domain|Method.code, adjust = "none")$emmeans)
#get scores for ranking visualizations
phase1.means<-as.data.frame(emmeans(model.phase1,pairwise ~domain|Method.code, adjust = "none")$emmeans)
# reorder and get the order variable to maintain facet-specific orders (otherwise it get screwy)
phase1.means <- phase1.means %>%
# 1. Remove grouping
ungroup() %>%
# 2. Arrange by
#   i.  facet group
#   ii. bar height
arrange(Method.code, emmean) %>%
# 3. Add order column of row numbers
mutate(order = row_number())
#get scores for ranking visualizations
phase1.means<-as.data.frame(emmeans(model.phase1,pairwise ~domain|Method.code, adjust = "none")$emmeans)
#analyses of phase 1  - MASE overall
#For models evaluating overall accuracy of the forecasted model, we will use forecasting type (purely theoretical, purely data-driven and hybrid models), forecasting domain as predictors, with MASE scores nested within teams.
model.phase1<-  lmer(MASE1_w1~domain*Method.code+(1|ResponseId), data=phase1)
#get scores for ranking visualizations
phase1.means<-as.data.frame(emmeans(model.phase1,pairwise ~domain|Method.code, adjust = "none")$emmeans)
# reorder and get the order variable to maintain facet-specific orders (otherwise it get screwy)
phase1.means <- phase1.means %>%
# 1. Remove grouping
ungroup() %>%
# 2. Arrange by
#   i.  facet group
#   ii. bar height
arrange(Method.code, emmean) %>%
# 3. Add order column of row numbers
mutate(order = row_number())
#lollipop chart
phase1.means %>% ggplot(aes(x=order, y=emmean, color=Method.code)) +
geom_point(size=3) +
geom_segment(aes(x=order,
xend=order,
y=0,
yend=emmean)) +facet_wrap(vars(Method.code), scales = "free")+
scale_x_continuous(   # Add categories to axis
breaks = phase1.means$order,
labels = phase1.means$domain)+theme_minimal(base_size = 14) +geom_hline(yintercept =1, linetype='dashed', color='red', 14)+theme(legend.position="none")+
scale_color_aaas()+ labs(colour = "Method",x="",y="Average MASE")
#errorbar charts, with scores ordered
phase1.means %>% ggplot(aes(x=order, y=emmean, color=Method.code)) +
geom_point(size=3) +
geom_errorbar(aes(ymin=lower.CL, ymax=upper.CL))+
facet_wrap(vars(Method.code), scales = "free", nrow=4)+theme_minimal() +geom_hline(yintercept =1, linetype='dashed', color='red', 14)+theme(legend.position="none")+
scale_x_continuous(   # Add categories to axis
breaks = phase1.means$order,
labels = phase1.means$domain,expand = c(0,0))+
scale_color_aaas()+ labs(colour = "Method",x="",y="Average MASE +/- 95% CI")
## EXAMINE EFFECTS OF UPDATING FOR PHASE I PREDICTIONS AMONG ACADEMICS
phase1_exp$updated<-ifelse(phase1_exp$revised==1,"update","no update")
model.phase1.update<-  lmer(MASE1_w1~domain*updated+(1|ResponseId), data=phase1_exp)
car::Anova(model.phase1.update,type="III") #no interaction, just a sig effect of domain
emmeans(model.phase1.update,pairwise ~updated|domain, adjust = "none") #nonsig
data.phase1.update<-as.data.frame(emmeans(model.phase1.update,pairwise ~updated|domain, adjust = "none")$emmeans) #nonsig
#visualize
data.phase1.update %>%
ggplot(aes(x = domain, y = emmean, colour = updated, fill=updated))+
geom_pointrange(aes(ymin=lower.CL, ymax=upper.CL), position=pd)+  theme_minimal(base_size = 14) +geom_hline(yintercept =1, linetype='dashed', color='red', 14)+
theme(legend.position="bottom") +scale_color_d3()+scale_fill_d3()+  scale_x_discrete(labels=labels, name="")+
labs(colour = "",fill="", x="",y="MASE (M +/- 95%CI)")
### overall MASE
#### first, confidence
model.phase1.conf<-  lmer(MASE1_w1~domain*confidence+(1|ResponseId), data=phase1_exp)
car::Anova(model.phase1.conf,type="III") #sig interaction
summ(model.phase1.conf, digits=4, center=T)
emtrends(model.phase1.conf,specs=pairwise~domain,var="confidence") #confidence plays a role for ideology republicans - the more confident the LOWER the mase scores and for LIFE SATISFACTION - the MORE confident the MORE error
##### second, team type - just academics
phase1_exp$teamS<-as.factor(ifelse(phase1_exp$team_size.coded>=6,3,ifelse(phase1_exp$team_size.coded<6&phase1_exp$team_size.coded>1,2,ifelse(phase1_exp$team_size.coded==1,1,NA))))
#####just count
model.phase1.team<-  lmer(MASE1_w1~domain*team_size.coded+(1|ResponseId), data=phase1_exp)
car::Anova(model.phase1.team,type="III") #sig interaction between domain and team size
summ(model.phase1.team, digits=4, center=T)
emtrends(model.phase1.team,specs=pairwise~domain,var="team_size.coded") #nothing
#####apriori defined groups
model.phase1.team3<-  lmer(MASE1_w1~domain*teamS+(1|ResponseId), data=phase1_exp)
car::Anova(model.phase1.team3,type="III") #no interaction between domain and team size
emmeans(model.phase1.team3,pairwise ~teamS|domain, adjust = "none") #nonsig
#### third, multidisciplinarity of the teams - just academics
phase1_exp$is_multidisciplinary<-ifelse(phase1_exp$discipline=="Multi-disciplinary",1,0)
model.phase1.multidis.team<-  lmer(MASE1_w1~domain*is_multidisciplinary+(1|ResponseId), data=phase1_exp)
car::Anova(model.phase1.multidis.team,type="III") #interdisciplinary did not matter
emmeans(model.phase1.multidis.team,pairwise ~is_multidisciplinary|domain, adjust = "none") #nonsig
#### fourth, Rating of 1-7 of whether participants considered themselves experts on the domain being predicted
model.phase1.subexpert<-  lmer(MASE1_w1~domain*subexpert+(1|ResponseId), data=phase1_exp)
car::Anova(model.phase1.subexpert,type="III") #
emtrends(model.phase1.subexpert,specs=pairwise~domain,var="subexpert") #
#### fifth, objective expertise based on publications in the domain (yes - no)
phase1_exp$objectivexpert<-ifelse(phase1_exp$pub==1,"Expert",ifelse(phase1_exp$pub==2,"Non Expert",NA))
model.phase1.obexpert<-  lmer(MASE1_w1~domain*objectivexpert+(1|ResponseId), data=phase1_exp)
car::Anova(model.phase1.obexpert,type="III") #
emmeans(model.phase1.obexpert,pairwise ~objectivexpert|domain, adjust = "none") #
#### six, number of predictors in the model
model.phase1.predictors<-  lmer(MASE1_w1~numpred*domain+(1|ResponseId), data=phase1_exp)
car::Anova(model.phase1.predictors,type="III") #npred matters!
emtrends(model.phase1.predictors,specs=~domain,var="numpred") #
sjPlot::plot_model(model.phase1.predictors,type="int")
#### six and a half, number of predictors in the model using CODED scores
model.phase1.predictors.coded<-  lmer(MASE1_w1~parameters_coded*domain+(1|ResponseId), data=phase1_exp)
car::Anova(model.phase1.predictors.coded,type="III") #npred matters!
emtrends(model.phase1.predictors.coded,specs=~domain,var="parameters_coded") #
sjPlot::plot_model(model.phase1.predictors.coded,type="int")
#### seventh, complexity of the model
model.phase1.complex<-  lmer(MASE1_w1~domain*Method.complex+(1|ResponseId), data=phase1_exp)
car::Anova(model.phase1.complex,type="III") #npred matters!
emtrends(model.phase1.complex,specs=~domain,var="Method.complex") #
model.phase1.complexF<-  lmer(MASE1_w1~domain*Method.complex.factor+(1|ResponseId), data=phase1_exp)
car::Anova(model.phase1.complexF,type="III") #npred matters!
emmeans(model.phase1.complexF,pairwise ~Method.complex.factor|domain, adjust = "none") #nonsig
sjPlot::plot_model(model.phase1.complexF,type="int")
##### presence of covid as a conditional
phase1_exp$covidconditional<-ifelse(phase1_exp$covidcondyn==0,"No",ifelse(phase1_exp$covidcondyn==1,"Yes",NA))
model.phase1.wcovid<-  lmer(MASE1_w1~domain*covidconditional+(1|ResponseId), data=phase1_exp)
car::Anova(model.phase1.wcovid,type="III") #sig interaction
emmeans(model.phase1.wcovid,pairwise ~covidconditional|domain, adjust = "none")
##### accuracy of thecovid as a conditional - using MASE
model.phase1.covid<-  lmer(MASE1_w1~domain*MASE1_covid+(1|ResponseId), data=phase1_exp)
car::Anova(model.phase1.covid,type="III") #
summ(model.phase1.covid, digits=4, center=T)
emtrends(model.phase1.covid,specs=pairwise~domain,var="MASE1_covid") #
#### Nine, counterfactuals (yes/no) when reflecting on predictions
model.phase1.counterfac<-  lmer(MASE1_w1~domain*CounterFactual_Presence_Final+(1|ResponseId), data=phase1_exp)
car::Anova(model.phase1.counterfac,type="III") #nothing
emmeans(model.phase1.counterfac,pairwise ~CounterFactual_Presence_Final|domain, adjust = "none") #
#### Nine and 1/2, number of counterfactuals when reflecting on predictions
model.phase1.counterfac.N<-  lmer(MASE1_w1~domain*counterNum+(1|ResponseId), data=phase1_exp)
car::Anova(model.phase1.counterfac.N,type="III") #nothing
emtrends(model.phase1.counterfac.N,specs=pairwise~domain,var="counterNum") #
#### Nine and 2/3, number of counterfactuals when reflecting on predictions
model.phase1.counterfac.covid<-  lmer(MASE1_w1~domain*as.factor(COVID.Final)+(1|ResponseId), data=phase1_exp)
car::Anova(model.phase1.counterfac.covid,type="III") #nothing
summ(model.phase1.counterfac.covid, digits=4, center=T)
emmeans(model.phase1.counterfac.covid,pairwise ~as.factor(COVID.Final)|domain, adjust = "none") #
#### Ten: experience with prior previous_tournaments
model.phase1.tournexperience<-  lmer(MASE1_w1~domain*as.factor(previous_tournament.coded)+(1|ResponseId), data=phase1_exp)
car::Anova(model.phase1.tournexperience,type="III") #nothing
emmeans(model.phase1.tournexperience,pairwise ~as.factor(previous_tournament.coded)|domain, adjust = "none") #nonsig
#### Eleven: only used data provided or also other data? - just those that used data
model.phase1.dataonly<-  lmer(MASE1_w1~domain*as.factor(DataOnly)+(1|ResponseId), data=phase1_exp)
car::Anova(model.phase1.dataonly,type="III") #3 way
emmeans(model.phase1.dataonly,pairwise ~as.factor(DataOnly)|domain, adjust = "none") #nonsig
model.phase1.team<-  lmer(MASE1_w1~domain+team_gender+team_education+team_Age+non_US+(1|ResponseId), data=phase1_exp)
car::Anova(model.phase1.team,type="III") ##% teams who did not have a PhD matters - i.e., fewer PhDs on the team=> greater MASE score (i.e., lower accuracy)
emtrends(model.phase1.team,specs=~domain,var="team_gender") #
emtrends(model.phase1.team,specs=~domain,var="team_education") #
emmeans(model.phase1.dataonly,pairwise ~team_education|domain, adjust = "none") #nonsig
emmeans(model.phase1.team,pairwise ~team_education|domain, adjust = "none") #nonsig

levels = c("ideolrep","ideoldem","polar",
"lifesat","negaffect","posaffect",
"iafric","iasian","igend",
"eafric","easian","egend" ))
#skewness test suggests that sqrt is the most reasonable transformation across the three metrics (esp. the first one) hence we will use it.
model.phase1.hist.mean.ratio<-  lmer(sqrt(MASE_ratio1)~domain+(1|team_name), data=phase1_exp_wbench)
Anova(model.phase1.hist.mean.ratio, test.statistic = "F")
emmeans(model.phase1.hist.mean.ratio,~domain, type="response")
#here are the results of the historical mean tests for Tournament 1
plot.t1.hist.mean<-plot(emmeans(model.phase1.hist.mean.ratio,~domain, type="response"),comparisons=F, color="black")+scale_y_discrete(labels=labeling, name="Historical Mean")+geom_vline(xintercept =1, linetype='dashed', color='black',14)+theme_minimal()+  labs(x="",shape="",color="", title="Tournament 1 (May 2020)")
model.phase1.randwalk.ratio<-  lmer(sqrt(MASE_ratio2)~domain+(1|team_name), data=phase1_exp_wbench)
Anova(model.phase1.randwalk.ratio, test.statistic = "F")
emmeans(model.phase1.randwalk.ratio,~domain, type="response")
#here are the results of the random walk tests for Tournament 1
plot.t1.randwalk<-plot(emmeans(model.phase1.randwalk.ratio,~domain, type="response"),comparisons=F, color="black")+scale_y_discrete(labels=labeling, name="Random Walk")+geom_vline(xintercept =1, linetype='dashed', color='black',14)+theme_minimal()+  labs(x="",shape="",color="", title="")
model.phase1.linreg.ratio<-  lmer(sqrt(MASE_ratio3)~domain+(1|team_name), data=phase1_exp_wbench)
Anova(model.phase1.linreg.ratio, test.statistic = "F")
emmeans(model.phase1.linreg.ratio,~domain, type="response")
#here are the results of the linear regression tests for Tournament 1
plot.t1.linreg<-plot(emmeans(model.phase1.linreg.ratio,~domain, type="response"),comparisons=F, color="black")+scale_y_discrete(labels=labeling, name="Linear Regression")+geom_vline(xintercept =1, linetype='dashed', color='black',14)+theme_minimal()+  labs(x="",shape="",color="", title="")
##examine if better than all 3 benchmarks
phase1_exp_wbench$phase1_exp_wbench_topBenchmark <- pmin(phase1_exp_wbench$'Benchmark 1',phase1_exp_wbench$'Benchmark 2',phase1_exp_wbench$'Benchmark 3')
phase1_exp_wbench$MASE_ratio1_topBenchmark<- phase1_exp_wbench$phase1_exp_wbench_topBenchmark/phase1_exp_wbench$MASE1_w1
#skewness test suggests that sqrt is the most reasonable transformation  hence we will use it.
model.phase1.topBenchmark.ratio<-  lmer(sqrt(MASE_ratio1_topBenchmark)~domain+(1|team_name), data=phase1_exp_wbench)
Anova(model.phase1.topBenchmark.ratio, test.statistic = "F")
emmeans(model.phase1.topBenchmark.ratio,~domain, type="response")
plot.t1.topBenchmark<-plot(emmeans(model.phase1.topBenchmark.ratio,~domain, type="response"),comparisons=F, color="black")+scale_y_discrete(labels=labeling, name="Top Naive Benchmark")+geom_vline(xintercept =1, linetype='dashed', color='black',14)+theme_minimal()+  labs(x="",shape="",color="", title="Tournament 1 (May 2020)")
#Tournament 2
phase2_exp_wbench<-dat_phase2 %>% left_join(pivot_wider(sim.w2 %>% select(domain,response, source),
names_from="source",values_from="response"))
phase2_exp_wbench$domain <- factor(phase2_exp_wbench$domain,      # Reordering group factor levels
levels = c("ideolrep","ideoldem","polar",
"lifesat","negaffect","posaffect",
"iafric","iasian","igend",
"eafric","easian","egend" ))
phase2_exp_wbench$MASE_ratio1<- phase2_exp_wbench$'Benchmark 1'/phase2_exp_wbench$MASE1_w2
phase2_exp_wbench$MASE_ratio2<- phase2_exp_wbench$'Benchmark 2'/phase2_exp_wbench$MASE1_w2
phase2_exp_wbench$MASE_ratio3<- phase2_exp_wbench$'Benchmark 3'/phase2_exp_wbench$MASE1_w2
#here we use logs, because skewness suggests that sqrt is not enough and logs do a good job across all three markers
model.phase2.hist.mean.ratio<-  lmer(log(MASE_ratio1)~domain+(1|team_name), data=phase2_exp_wbench)
Anova(model.phase2.hist.mean.ratio, test.statistic = "F")
emmeans(model.phase2.hist.mean.ratio,~domain, type="response")
#here are the results of the historical mean tests for Tournament 2
plot.t2.hist.mean<-plot(emmeans(model.phase2.hist.mean.ratio,~domain, type="response"),comparisons=F, color="black")+scale_y_discrete(name="")+geom_vline(xintercept =1, linetype='dashed', color='black',14)+theme_minimal()+  labs(x="",shape="",color="", title="Tournament 2 (Nov 2020)")+theme(axis.text.y=element_blank())
model.phase2.randwalk.ratio<-  lmer(log(MASE_ratio2)~domain+(1|team_name), data=phase2_exp_wbench)
Anova(model.phase2.randwalk.ratio, test.statistic = "F")
emmeans(model.phase2.randwalk.ratio,~domain, type="response")
#here are the results of the random walk tests for Tournament 2
plot.t2.randwalk<-plot(emmeans(model.phase2.randwalk.ratio,~domain, type="response"),comparisons=F, color="black")+scale_y_discrete(name="")+geom_vline(xintercept =1, linetype='dashed', color='black',14)+theme_minimal()+  labs(x="",shape="",color="", title="")+theme(axis.text.y=element_blank())
model.phase2.linreg.ratio<-  lmer(log(MASE_ratio3)~domain+(1|team_name), data=phase2_exp_wbench)
Anova(model.phase2.linreg.ratio, test.statistic = "F")
emmeans(model.phase2.linreg.ratio,~domain, type="response")
#here are the results of the linear regression tests for Tournament 2
plot.t2.linreg<-plot(emmeans(model.phase2.linreg.ratio,~domain, type="response"),comparisons=F, color="black")+scale_y_discrete(name="")+geom_vline(xintercept =1, linetype='dashed', color='black',14)+theme_minimal()+  labs(x="",shape="",color="", title="")+theme(axis.text.y=element_blank())
##examine if better than all 3 benchmarks
phase2_exp_wbench$phase2_exp_wbench_topBenchmark <- pmin(phase2_exp_wbench$'Benchmark 1',phase2_exp_wbench$'Benchmark 2',phase2_exp_wbench$'Benchmark 3')
phase2_exp_wbench$MASE_ratio2_topBenchmark<- phase2_exp_wbench$phase2_exp_wbench_topBenchmark/phase2_exp_wbench$MASE1_w2
#skewness test suggests that sqrt is the most reasonable transformation  hence we will use it.
model.phase2.topBenchmark.ratio<-  lmer(sqrt(MASE_ratio2_topBenchmark)~domain+(1|team_name), data=phase2_exp_wbench)
Anova(model.phase2.topBenchmark.ratio, test.statistic = "F")
emmeans(model.phase2.topBenchmark.ratio,~domain, type="response")
plot.t2.topBenchmark<-plot(emmeans(model.phase2.topBenchmark.ratio,~domain, type="response"),comparisons=F, color="black")+scale_y_discrete(labels=labeling, name="Top Naive Benchmark")+geom_vline(xintercept =1, linetype='dashed', color='black',14)+theme_minimal()+  labs(x="",shape="",color="", title="Tournament 2 (Nov 2020)")
#combine all graphs
figs2<-ggarrange(plot.t1.hist.mean,plot.t2.hist.mean,
plot.t1.randwalk,plot.t2.randwalk,
plot.t1.linreg, plot.t2.linreg,  ncol=2, nrow=3,widths=c(1.3,1))
figs2
# Chunk 13
#count how many domains per person
phase1_exp<-phase1_exp %>%group_by(team_name) %>%  mutate(n_domains = n())
phase1_exp$Domain_Publications<-ifelse(phase1_exp$pub==1,1,ifelse(phase1_exp$pub==2,0,NA))
#count how many domains per person
dat_phase2<-dat_phase2 %>%group_by(team_name) %>%  mutate(n_domains = n())
dat_phase2$Domain_Publications<-ifelse(dat_phase2$pub==1,1,ifelse(dat_phase2$pub==2,0,NA))
#####################
#create subsets for tournament 1 and for tournament 2 to use in analyses here and later for covariate analyses below
#####################
subset1<- phase1_exp %>% ungroup() %>% select(MASE1_w1,domain,Method.code,ResponseId,team_name,covidcondyn,CounterFactual_Presence_Final,Method.complex,parameters_coded,n_domains,multi_dis.factor,team_discipline.coded,non_US,team_size.coded,team_gender,team_education,confidence,subexpert,Domain_Publications,previous_tournament.coded,TournamentStart) %>% mutate(inaccuracy = MASE1_w1,phase = "first")
subset2<- dat_phase2 %>% ungroup() %>% select(MASE1_w2,domain,Method.code,ResponseId,team_name,covidcondyn,CounterFactual_Presence_Final,Method.complex,parameters_coded,n_domains,multi_dis.factor,team_discipline.coded,non_US,team_size.coded,team_gender,team_education,confidence,subexpert,Domain_Publications,previous_tournament.coded,TournamentStart) %>% mutate(inaccuracy = MASE1_w2,phase = "second")
##compare effects by domain for each tournament
##REPORTED IN MAIN TEXT####
subset1.model<-  lmer(log(inaccuracy)~domain+(1|team_name), data=subset1)
car::Anova(subset1.model,type="III", test.statistic="F") #sig effect
emmeans(subset1.model,~domain, type="response")
partR2(subset1.model)
#rsq = 0.4498
subset2.model<-  lmer(log(inaccuracy)~domain+(1|team_name), data=subset2)
car::Anova(subset2.model,type="III", test.statistic="F") #sig effect
emmeans(subset2.model,~domain, type="response")
partR2(subset2.model)
#rsq = 0.2909
###########################
##############################
#combine tournament 1 and tournament 2 subsets for later analyses with covariates
#BEGINNING
##############################
both.sets<-bind_rows(subset1,subset2)
both.sets$covidconditional<-ifelse(both.sets$covidcondyn==1,1,0)
both.sets$covidconditional[is.na(both.sets$covidconditional)]<-0
both.sets$Method.complex[is.na(both.sets$Method.complex)]<-1 #simple when no extra info is provided, because the rest {number of parameters et.) suggests no extra factors considered}
both.sets$multi_dis.factor[is.na(both.sets$multi_dis.factor)]<-"Single domain expertise" #(setting is NA to non multidisciplinary)
both.sets$team_discipline.coded[is.na(both.sets$team_discipline.coded)]<-5 #(setting is NA to other)
both.sets$team_discipline.datasci<-ifelse(both.sets$team_discipline.coded==3,1,0)
both.sets$team_discipline.SBsci<-ifelse(both.sets$team_discipline.coded==1,1,ifelse(both.sets$team_discipline.coded==2,1,0))
#add complexity
both.sets<-both.sets %>% left_join(complexity)
both.sets$sd_hist<-ifelse(both.sets$phase=="first",both.sets$sd_hist_w1,both.sets$sd_hist_w2)
both.sets$mad_hist<-ifelse(both.sets$phase=="first",both.sets$mad_hist_w1,both.sets$mad_hist_w2)
both.sets$perp_entropy_hist<-ifelse(both.sets$phase=="first",both.sets$perp_entropy_hist_w1,both.sets$perp_entropy_hist_w2)
both.sets$sd<-ifelse(both.sets$phase=="first",both.sets$sd_w1,both.sets$sd_w2)
both.sets$mad<-ifelse(both.sets$phase=="first",both.sets$mad_w1,both.sets$mad_w2)
both.sets$perp_entropy<-ifelse(both.sets$phase=="first",both.sets$perp_entropy_w1,both.sets$perp_entropy_w2)
#add domain differences in complexity between waves (just supplementary interests)
both.sets$sd_hist_diff<-both.sets$sd_hist_w2-both.sets$sd_hist_w1
both.sets$mad_hist_diff<-both.sets$mad_hist_w2-both.sets$mad_hist_w1
both.sets$perp_entropy_hist_diff<-both.sets$perp_entropy_hist_w2-both.sets$perp_entropy_hist_w1
both.sets$sd_diff<-both.sets$sd_w2-both.sets$sd_w1
both.sets$mad_diff<-both.sets$mad_w2-both.sets$mad_w1
both.sets$perp_entropy_diff<-both.sets$perp_entropy_w2-both.sets$perp_entropy_w1
##############################
#combine tournament 1 and tournament 2 subsets for later analyses with covariates
#END
##############################
#############################
#analyze comparison of tournament 1 to tournament 2, REPORTED IN THE MAIN TEXT
#BEGINNING
#############################
both.sets.model<-  lmer(log(inaccuracy)~phase+(1|team_name), data=both.sets)
car::Anova(both.sets.model,type="III", test.statistic="F") #sig effect
emmeans(both.sets.model,~phase, type="response")
partR2(both.sets.model)
#effect size part rsq  0.0628
#####################
#supplementary analyses - comparison of tournament 1 versus tournament 2 by domain
#####################
both.sets.model.by.domain<-  lmer(log(inaccuracy)~phase*domain+(1|team_name), data=both.sets)
car::Anova(both.sets.model.by.domain,type="III", test.statistic="F") #sig effect
emmeans(both.sets.model.by.domain,~phase|domain, type="response")
emmeans(both.sets.model.by.domain,pairwise~phase|domain, type="response")
t.comparison.effects<-as.data.frame(emmeans(both.sets.model.by.domain,pairwise~phase|domain, type="response")$emmeans)
t.comparison<-as.data.frame(emmeans(both.sets.model.by.domain,pairwise~phase|domain, type="response")$contrasts)
#####################
#analyses of tournament 1 versus tournament 2 with covariates
#####################
both.sets.model.cov<-  lmer(log(inaccuracy)~phase+domain+
n_domains+team_discipline.datasci+team_discipline.SBsci+multi_dis.factor+team_size.coded+team_gender+team_education+Domain_Publications+previous_tournament.coded+(1|team_name), data=both.sets)
car::Anova(both.sets.model.cov,type="III", test.statistic="F") #sig effect
emmeans(both.sets.model.cov,~phase, type="response")
partR2(both.sets.model.cov, partvars =
c("phase","domain"))
#ergo, part rsq for phase itself remains 0.0617
# Chunk 14
##MAIN TEXT FIGURE SHOWING RANKING AND DOMAIN'S MASE, AS WELL AS SHOWING WHICH DIFFERENCES ARE SIG
#get ranking of scores among academics in May and November
median.MASE.t1$phase<-"first"
median.MASE.t2$phase<-"second"
median.MASE.t1$Wave<-"First Tournament\nMay 2020"
median.MASE.t2$Wave<-"Second Tournament\nNov 2020"
median.ranks<-bind_rows(median.MASE.t1,median.MASE.t2)
median.ranks$Domain[median.ranks$domain=="eafric"]<-"Exp. Afr.-Am. Bias"
median.ranks$Domain[median.ranks$domain=="easian"]<-"Exp. Asian-Am. Bias"
median.ranks$Domain[median.ranks$domain=="egend"]<-"Exp. Gender Bias"
median.ranks$Domain[median.ranks$domain=="iafric"]<-"Imp. Afr.-Am. Bias"
median.ranks$Domain[median.ranks$domain=="iasian"]<-"Imp. Asian-Am. Bias"
median.ranks$Domain[median.ranks$domain=="ideoldem"]<-"Democrat. Support"
median.ranks$Domain[median.ranks$domain=="ideolrep"]<-"Republic. Support"
median.ranks$Domain[median.ranks$domain=="igend"]<-"Imp. Gender Bias"
median.ranks$Domain[median.ranks$domain=="lifesat"]<-"Life Satisfaction"
median.ranks$Domain[median.ranks$domain=="polar"]<-"Polarization"
median.ranks$Domain[median.ranks$domain=="negaffect"]<-"Negative Affect"
median.ranks$Domain[median.ranks$domain=="posaffect"]<-"Positive Affect"
median.ranks<-median.ranks %>% left_join(t.comparison.effects) %>% left_join(t.comparison %>% select(domain,ratio,t.ratio,p.value)) #add the sig testing from the tournament comparisons, incl ratio size and p-values
#NOTE: here we have median ranks per domain, but also the estimates scores from multi-level models accounting for multiple predictions by different scientist groups. Due to this dependence in the data, we use the latter estimates.
median.ranks$sig<-ifelse(median.ranks$p.value<.05,"eff","noeff")
median.ranks$MASE<-round(median.ranks$response,2) #two decimals
median.ranks$ranksize<-round(median.ranks$ratio,2) #two decimals
newggslopegraph(dataframe = median.ranks,
Times = Wave,
Measurement = MASE,
Grouping = Domain,LineThickness = 2,
WiderLabels=T,
Title = "Which domains are harder to predict?",TitleJustify = "center",
SubTitle = NULL,
Caption = "Ranking based on MASE scores per domain",
ThemeChoice="ipsum")+scale_color_d3(palette = "category20")+geom_line(aes(linetype=sig, color="black",alpha=1))
# Chunk 15: ranking in phase 1 and complexity
##test reported in MAIN TEXT showing that domain differences in forecasting accuracy corresponded to differences in the complexity of historical data
#covert to wide
median.ranks.wide<-median.ranks %>% select(domain:phase,MASE) %>%
pivot_wider(names_from="phase",values_from=c("MASE","MASE_med"))
rank_complex_wide<-median.ranks.wide%>%left_join(complexity)
#For complexity,we used SD and MAD. Additionally, I consider permutation_entropy, which is Ra substitution the Shannon entropy with a monoparametric entropy.
rank_complex_w1<-median.MASE.t1%>%left_join(complexity)
rank_complex_w2<-median.MASE.t2%>%left_join(complexity)
rank_complex<-median.ranks%>%left_join(complexity)
#the scores below include both complexity markers (SD, MAD, and supplementary entropy) for historical data, as well as the data across the 12 months of the tournament. For the main text analyses we focus on historical complexity
correlation::correlation(rank_complex%>%filter(phase=="first") %>%
select(MASE_med,sd_hist_w1,mad_hist_w1,perp_entropy_hist_w1,sd_w1,mad_w1,perp_entropy_w1), p_adjust="none", ranktransform=T)
correlation::correlation(rank_complex%>%filter(phase=="second") %>%
select(MASE_med,sd_hist_w2,mad_hist_w2,perp_entropy_hist_w2,sd_w2,mad_w2,perp_entropy_w2), p_adjust="none", ranktransform=T)
#add difference scores in complexity to the dataset
#these are additional analyses to check the differences in complexity and differences in accuracy between tournaments
rank_complex$sd_hist_diff<-rank_complex$sd_hist_w2-rank_complex$sd_hist_w1
rank_complex$mad_hist_diff<-rank_complex$mad_hist_w2-rank_complex$mad_hist_w1
rank_complex$perp_entropy_hist_diff<-rank_complex$perp_entropy_hist_w2-rank_complex$perp_entropy_hist_w1
rank_complex$sd_diff<-rank_complex$sd_w2-rank_complex$sd_w1
rank_complex$mad_diff<-rank_complex$mad_w2-rank_complex$mad_w1
rank_complex$perp_entropy_diff<-rank_complex$perp_entropy_w2-rank_complex$perp_entropy_w1
rank_complex$wave<-ifelse(rank_complex$phase=="first",0,1)
domain.SD.change<-  lmer(MASE~wave*sd_hist_diff+(1|Domain), data=rank_complex)
car::Anova(domain.SD.change,type="III", test.statistic="F") #sig effect
emtrends(domain.SD.change,specs=~wave,var="sd_hist_diff") #
interactions::sim_slopes(domain.SD.change,pred="wave",modx="sd_hist_diff", digits=4)
#this interaction shows that when change in SD is high (domains become more variable at t2 compared to t1), there is no difference in inaccuracy
domain.MAD.change<-  lmer(MASE~wave*mad_hist_diff+(1|Domain), data=rank_complex)
car::Anova(domain.MAD.change,type="III", test.statistic="F") #sig effect
emtrends(domain.MAD.change,specs=~wave,var="mad_hist_diff") #
interactions::sim_slopes(domain.MAD.change,pred="wave",modx="mad_hist_diff", digits=4)
#this interaction shows that when change in MAD is high (domains become more variable at t2 compared to t1), there is no difference in inaccuracy between t1 and t2
##examine difference scores via corr
rank_complex_wide$sd_hist_diff<-rank_complex_wide$sd_hist_w2-rank_complex_wide$sd_hist_w1
rank_complex_wide$mad_hist_diff<-rank_complex_wide$mad_hist_w2-rank_complex_wide$mad_hist_w1
rank_complex_wide$perp_entropy_hist_diff<-rank_complex_wide$perp_entropy_hist_w2-rank_complex_wide$perp_entropy_hist_w1
rank_complex_wide$sd_diff<-rank_complex_wide$sd_w2-rank_complex_wide$sd_w1
rank_complex_wide$mad_diff<-rank_complex_wide$mad_w2-rank_complex_wide$mad_w1
rank_complex_wide$perp_entropy_diff<-rank_complex_wide$perp_entropy_w2-rank_complex_wide$perp_entropy_w1
rank_complex_wide$MASE_diff<-rank_complex_wide$MASE_second-rank_complex_wide$MASE_first
rank_complex_wide$MASE_med_diff<-rank_complex_wide$MASE_med_second-rank_complex_wide$MASE_med_first
correlation::correlation(rank_complex_wide%>%
select(MASE_diff,MASE_med_diff,sd_hist_diff,mad_hist_diff,perp_entropy_hist_diff,sd_diff,mad_diff,perp_entropy_diff), p_adjust="none", ranktransform=T)
#added info about change in variability correlating to changes in accuracy.
# Chunk 16
#turn to long format (firstmonths and lastmonths MASE data)
data.t1.t2.exp.long<- phase1_exp %>% ungroup() %>%pivot_longer(cols=c("MASE1_w1_firstmonths","MASE1_w1_lastmonths"), names_to="Time",values_to="MASE")
model.t1.t2<-lmer(log(MASE)~Time+domain+(1|team_name), data=data.t1.t2.exp.long)
summ(model.t1.t2)
car::Anova(model.t1.t2,type="III",test.statistic="F") #sig interaction!
emmeans(model.t1.t2,~Time, , type="response")
partR2(model.t1.t2, partvars =
c("Time","domain"))
#NEXT ANALYSES: compare last months from the T1 to T2
##turn to long format (lastmonths T1 an t2 MASE data)
subset1.lastsix<- phase1_exp %>% ungroup() %>% select(MASE1_w1_lastmonths,domain,Method.code,ResponseId,team_name,covidcondyn,CounterFactual_Presence_Final,Method.complex,parameters_coded,n_domains,multi_dis.factor,team_discipline.coded,team_size.coded,team_gender,team_education,confidence,subexpert,Domain_Publications,previous_tournament.coded, TournamentStart) %>% mutate(inaccuracy = MASE1_w1_lastmonths,phase = "first")
#combine
both.sets.lastsix<-bind_rows(subset1.lastsix,subset2)
#test
model.t1.t2.lastsix<-lmer(log(inaccuracy)~phase+domain+(1|team_name), data=subset(both.sets.lastsix,TournamentStart=="May"))
car::Anova(model.t1.t2.lastsix,type="III",test.statistic="F")
emmeans(model.t1.t2.lastsix,specs = trt.vs.ctrl ~phase,  type="response",adjust = "fdr")
partR2(model.t1.t2.lastsix, partvars =
c("phase","domain"))
# Chunk 17: inaccuracy on odd and even month - stability of inaccuracy
#to assess and protect against the possibility that forecasting models are accurate by chance (in the same way that some investing strategies can “get lucky” in a particular time point without actually being better than other strategies), we used subsets of the data (odd and event months) to determine whether model accuracy in one subset of predictions (ranking of model performance across odd months) correlates with model accuracy in the other subset (ranking of model performance across even months).
dat_long_phase1_exp<-(subset(dat_long_phase1, isExpert.factor == 'Academic'))
dat_long_phase1_exp_wide_by_month<-dat_long_phase1_exp %>%dplyr::select(domain,team_name,Month,value.dif) %>%  pivot_wider(names_from=c(Month),values_from=c(value.dif))
dat_long_phase1_exp_wide_by_month$odd_month_inaccuracy=rowMeans(dat_long_phase1_exp_wide_by_month[c("1","3","5","7","9","11")],na.rm=T)
dat_long_phase1_exp_wide_by_month$even_month_inaccuracy=rowMeans(dat_long_phase1_exp_wide_by_month[c("2","4","6","8","10","12")],na.rm=T)
#correlations by domain
dat_long_phase1_exp_wide_by_month %>%select(domain,odd_month_inaccuracy,even_month_inaccuracy) %>%
group_by(domain) %>%
correlation::correlation(ranktransform =T)
#multilevel
dat_long_phase1_exp_wide_by_month %>%select(domain,odd_month_inaccuracy,even_month_inaccuracy) %>%
correlation::correlation(multilevel=T,ranktransform=T)
###PHASE 2
dat_long$Month7<-dat_long$Month-7
dat_long_phase2<-dat_long %>%filter(!(phase == 1 & revised == 1)& Method.code!="Ground Truth"& Method.code!="Naive-linear"&Method.code!="Naive-rfw" & Month %in% c(7,8,9,10,11,12))
dat_long_phase2_exp_wide_by_month<-dat_long_phase2 %>%dplyr::select(domain,team_name,Month7,value.dif) %>%  pivot_wider(names_from=c(Month7),values_from=c(value.dif))
dat_long_phase2_exp_wide_by_month$odd_month_inaccuracy=rowMeans(dat_long_phase2_exp_wide_by_month[c("1","3","5")],na.rm=T)
dat_long_phase2_exp_wide_by_month$even_month_inaccuracy=rowMeans(dat_long_phase2_exp_wide_by_month[c("2","4","0")],na.rm=T)
#correlations by domain
dat_long_phase2_exp_wide_by_month %>%select(domain,odd_month_inaccuracy,even_month_inaccuracy) %>%
group_by(domain) %>%
correlation::correlation(ranktransform =T)
#multilevel
dat_long_phase2_exp_wide_by_month %>%select(domain,odd_month_inaccuracy,even_month_inaccuracy) %>%
correlation::correlation(multilevel=T,ranktransform=T)
# Chunk 18
#analyses of phase 1  - MASE overall
#For models evaluating overall accuracy of the forecasted model, we will use forecasting type (purely theoretical, purely data-driven and hybrid models), forecasting domain as predictors, with MASE scores nested within teams.
#first, data the proper subset for Tournament 2
dat_phase2<-academic_only %>%filter(!(phase == 1 & revised == 1)) #just academics
######################################
#what is the percentage using different method?
#Tournament 1:
prop.table(table(phase1_exp$Method.code))
#Tournament 2:
prop.table(table(dat_phase2$Method.code))
table(phase2$Method.code)
#SUPPLEMENTARY FIGURE showing differences in percentages of each category by domain
######################################
perc.by.domain.phase1<-phase1_exp %>%
group_by(domain,Method.code) %>%
summarise(n = n()) %>%
mutate(perc = round(n / sum(n)*100),2) %>%
ggplot(aes(x = "", y = perc, fill = Method.code)) +
geom_col(color = "black") +
geom_label(aes(label = perc),
color = "white",
position = position_stack(vjust = 0.5),
show.legend = FALSE) +scale_fill_jama()+labs(fill="")+
coord_polar(theta = "y")+theme_void()+facet_wrap(~domain, nrow = 4, labeller=labeller(domain=labels))+theme(legend.position="bottom")
perc.by.domain.phase1
#Tournament 2
perc.by.domain.phase2<-dat_phase2 %>%
group_by(domain,Method.code) %>%
summarise(n = n()) %>%
mutate(perc = round(n / sum(n)*100),2) %>%
ggplot(aes(x = "", y = perc, fill = Method.code)) +
geom_col(color = "black") +
geom_label(aes(label = perc),
color = "white",
position = position_stack(vjust = 0.5),
show.legend = FALSE) +scale_fill_jama()+labs(fill="")+
coord_polar(theta = "y")+theme_void()+facet_wrap(~domain, nrow = 4, labeller=labeller(domain=labels))+theme(legend.position="bottom")
#combine plots
#plot percentages of different forecasting method choices by domain for tournament 1 and tournament 2 (i.e., put them together)
cowplot::plot_grid(perc.by.domain.phase1,perc.by.domain.phase2,labels=c("1st Tournament","2nd Tournament"), label_size = 10,
align = "v")
####################################
#examine analyses of forecasting method choice on accuracy
####################################
#recorder levels of the domains (to use later)
dat_long$domain <- factor(dat_long$domain,      # Reordering group factor levels
levels = c("egend","easian","eafric",
"igend","iasian","iafric",
"posaffect","negaffect","lifesat",
"polar","ideoldem","ideolrep"))
#Tournament 1: run models
model.phase1.across.domains<-  lmer(log(MASE1_w1)~Method.code+domain+(1|ResponseId), data=subset(phase1,isExpert.factor=="Academic"))
car::Anova(model.phase1.across.domains,type="III", test.statistic="F")
partR2(model.phase1.across.domains, partvars =
c("Method.code","domain"))
data.phase1.MASE.total<-as.data.frame(emmeans(model.phase1.across.domains,pairwise ~Method.code, type = "response", adjust = "none")$emmeans)
#next run Tournament 2 models
data.phase2.model.across.domains<-  lmer(log(MASE1_w2)~Method.code+domain+(1|ResponseId), data=dat_phase2)
car::Anova(data.phase2.model.across.domains,type="III", test.statistic="F") #sig interaction!
data.phase2.MASE.total<-as.data.frame(emmeans(data.phase2.model.across.domains,pairwise ~Method.code, type = "response", adjust = "none")$emmeans)
partR2(data.phase2.model.across.domains, partvars =
c("Method.code","domain"))
## we test if forecasts that considered historical data as part of the forecast modelling were more accurate than models that did not - MAIN TEXT
#i.e., EXAMINE ONLY ACADEMICS, USING CONTRAST OF THEORY vs. DATA.HYBRID
### Tournament 1
phase1_exp$method.contrast<-ifelse(phase1_exp$Method.code=='Intuition/Theory',0,1)
model.phase1.contrast<-  lmer(log(MASE1_w1)~method.contrast+domain+(1|ResponseId), data=phase1_exp)
car::Anova(model.phase1.contrast,type="III", test.statistic="F") #sig domain effect,  and sig interaction
summ(model.phase1.contrast, digits=4) #get effect size for the overall model
partR2(model.phase1.contrast, partvars =
c("method.contrast","domain"))
### Tournament 2
dat_phase2$method.contrast<-ifelse(dat_phase2$Method.code=='Intuition/Theory',0,1)
model.phase2.contrast<-  lmer(log(MASE1_w2)~method.contrast+domain+(1|ResponseId), data=dat_phase2)
car::Anova(model.phase2.contrast,type="III", test.statistic="F") #sig domain effect,  and sig interaction
summ(model.phase2.contrast, digits=4) #get effect size for the overall model
partR2(model.phase2.contrast, partvars =
c("method.contrast","domain"))
## Test if model comparison effects were qualified by significant model type X domain interaction
### Tournament 1
model.phase1.contrast.by.domain<-  lmer(log(MASE1_w1)~method.contrast*domain+(1|ResponseId), data=phase1_exp)
car::Anova(model.phase1.contrast.by.domain, type=3, test.statistic="F")
emmeans(model.phase1.contrast.by.domain,pairwise ~method.contrast|domain, adjust = "fdr",type="response" )
partR2(model.phase1.contrast.by.domain, partvars =
c("method.contrast","domain:method.contrast","domain"))
write.csv(emmeans(model.phase1.contrast.by.domain,pairwise ~method.contrast|domain, adjust = "fdr",type="response" )
$contrasts,"contrast1.csv") #for the table in supplement
### Tournament 2
model.phase2.contrast.by.domain<-  lmer(log(MASE1_w2)~method.contrast*domain+(1|ResponseId), data=dat_phase2)
car::Anova(model.phase2.contrast.by.domain, type=3, test.statistic="F")
emmeans(model.phase2.contrast.by.domain,pairwise ~method.contrast|domain, adjust = "fdr",type="response" )
partR2(model.phase2.contrast.by.domain, partvars =
c("method.contrast","domain:method.contrast","domain"))
write.csv(emmeans(model.phase2.contrast.by.domain,pairwise ~method.contrast|domain, adjust = "none",type="response" )
$contrasts,"contrast2.csv") #for table in the supplement
## supplementary model with all three forecasting methods*domain interaction is below. We use it to get estimates for modelling by domain by method
### Tournament 1
model.phase1<-  lmer(log(MASE1_w1)~domain*Method.code+(1|ResponseId), data=subset(phase1,isExpert.factor=="Academic"))
car::Anova(model.phase1,type="III") #sig interaction!
summ(model.phase1, digits=4)
emmeans(model.phase1,trt.vs.ctrl ~Method.code|domain, adjust = "none") #lay vs. rest
emmeans(model.phase1,pairwise ~Method.code|domain, adjust = "none")
partR2(model.phase1, partvars =
c("domain","domain:Method.code","domain"))
data.phase1.MASE<-as.data.frame(emmeans(model.phase1,pairwise ~Method.code|domain, type = "response", adjust = "none")$emmeans)
### Tournament 2
data.phase2.model<-  lmer(log(MASE1_w2)~domain*Method.code+(1|ResponseId), data=dat_phase2)
car::Anova(data.phase2.model,type="III") #sig interaction!
partR2(data.phase2.model, partvars =
c("domain","domain:Method.code","domain"))
data.phase2.MASE<-as.data.frame(emmeans(data.phase2.model, pairwise~Method.code|domain, adjust = "none", type = "response")$emmeans) #backtransformed to the original scale
## Supplementary analyses to exmaine if data-free forecasts of social scientists were not better than lay estimates, in Tournament 1
###EXAMINE ONLY ACADEMICS, USING CONTRAST OF THEORY vs. DATA.HYBRID
phase1$method.contrast.layppl[phase1$Method.code=='Intuition/Theory']<-"Sci data-free"
phase1$method.contrast.layppl[phase1$Method.code=='Lay People']<-"lay people"
phase1$method.contrast.layppl[phase1$Method.code=='Data-Driven']<-"Sci data-incl."
phase1$method.contrast.layppl[phase1$Method.code=='Hybrid']<-"Sci data-incl."
phase1$MASE1_w1_log<-log(phase1$MASE1_w1) #this this to get emmeans-based effect size Cohen's d for pairwise comparisons
model.phase1.contrast.lay<-  lmer(MASE1_w1_log~method.contrast.layppl+(1|ResponseId), data=phase1)
car::Anova(model.phase1.contrast.lay,type="III") #sig domain effect,  and sig interaction
emmeans(model.phase1.contrast.lay,specs = trt.vs.ctrl ~method.contrast.layppl, adjust = "fdr",type="response" )
#significant difference between academics who used data and lay people, but not between academics who did not use data and lay people
( EMM = emmeans(model.phase1.contrast.lay, "method.contrast.layppl") )
pairs(EMM)
eff_size(EMM,sigma = sigma(model.phase1.contrast.lay), edf =df.residual(model.phase1.contrast.lay) ) #using the smallest DF among the three
############SOME EXTRA FIGURES: NOT USED IN THE MANUSCRIPT OR SUPPLEMENT##################
#########BEGINNING###################
###Tournament 1
#arrange in descending order based on MASE w2 of academics
data.phase1.MASE$domain<-factor(data.phase1.MASE$domain,levels=c("ideolrep","negaffect","ideoldem","polar","iafric","lifesat","eafric","easian","egend","iasian","igend","posaffect"))
data.phase1.MASE %>%
ggplot(aes(x = domain, y = response, colour = Method.code, fill=Method.code))+
geom_pointrange(aes(ymin=lower.CL, ymax=upper.CL), position=pd)+  theme_minimal(base_size = 14) +geom_hline(yintercept =1, linetype='dashed', color='red', 14)+coord_flip()+
theme(legend.position="bottom") +scale_color_d3()+scale_fill_d3()+  scale_x_discrete(labels=labeling, name="")+
labs(colour = "",fill="", x="",y="MASE (M +/- 95%CI)")
data.phase1.MASE.total %>%
ggplot(aes(x = Method.code, y = response, colour = Method.code, fill=Method.code))+
geom_pointrange(aes(ymin=lower.CL, ymax=upper.CL), position=pd)+  theme_minimal(base_size = 14) +geom_hline(yintercept =1, linetype='dashed', color='red', 14)+coord_flip()+
theme(legend.position="bottom") +scale_color_d3()+scale_fill_d3()+  scale_x_discrete(labels=labeling, name="")+
labs(colour = "",fill="", x="",y="MASE (M +/- 95%CI)")
### Tournament 2
data.phase2.MASE %>%
ggplot(aes(x = domain, y = response, colour = Method.code, fill=Method.code))+
geom_pointrange(aes(ymin=lower.CL, ymax=upper.CL), position=pd)+  theme_minimal(base_size = 14) +geom_hline(yintercept =1, linetype='dashed', color='red', 14)+coord_flip()+
theme(legend.position="bottom") +scale_color_d3()+scale_fill_d3()+  scale_x_discrete(labels=labeling, name="")+
labs(colour = "",fill="", x="",y="MASE (M +/- 95%CI)")
data.phase2.MASE.total %>%
ggplot(aes(x = Method.code, y = response, colour = Method.code, fill=Method.code))+
geom_pointrange(aes(ymin=lower.CL, ymax=upper.CL), position=pd)+  theme_minimal(base_size = 14) +geom_hline(yintercept =1, linetype='dashed', color='red', 14)+coord_flip()+
theme(legend.position="bottom") +scale_color_d3()+scale_fill_d3()+  scale_x_discrete(labels=labeling, name="")+
labs(colour = "",fill="", x="",y="MASE (M +/- 95%CI)")
############SOME EXTRA FIGURES: NOT USED IN THE MANUSCRIPT OR SUPPLEMENT##################
#########END###################
###CREATE FIGURE FOR THE MAIN TEXT
data.phase1.MASE.total$Wave<-"First Tournament (May 2020)"
data.phase2.MASE.total$Wave<-"Second Tournament (Nov 2020)"
#combine
means.compare.by.method<-bind_rows(data.phase1.MASE.total,data.phase2.MASE.total)
means.compare.by.method$Method<-means.compare.by.method$Method.code #create a copy to port values to
means.compare.by.method$Method<-c('Data-Driven\n51%','Hybrid\n7%','Intuition/\nTheory\n42%','Data-Driven\n53%','Hybrid\n8%','Intuition/\nTheory\n39%')
#arrange in descending order based on MASE w2 of academics
means.compare.by.method$Wave<-factor(means.compare.by.method$Wave,levels=c("First Tournament (May 2020)","Second Tournament (Nov 2020)"))
#plot figure
means.compare.by.method %>%
ggplot(aes(x = Method, y = response, color = Method.code))+
geom_pointrange(aes(ymin=lower.CL, ymax=upper.CL), position=pd)+  theme_minimal(base_size = 14)+geom_hline(yintercept =1, linetype='dashed', color='red',14)+theme(legend.position="none")+scale_color_futurama()+  labs(y="MASE (M +/- 95%CI)",x="",shape="",color="")+ facet_wrap(~ Wave, scales = "free_x")
# Chunk 19
#examine effects of covariates
both.sets$inaccuracy_log<-log(both.sets$inaccuracy)
both.sets$Multidisciplinary<-ifelse(both.sets$multi_dis.factor=="Single domain expertise",0,1)
both.sets$covidconditional[is.na(both.sets$covidconditional)]<-0
both.sets$Method.complex[is.na(both.sets$Method.complex)]<-1 #simple when no extra info is provided, because the rest {number of parameters et.) suggests no extra factors considered}
both.sets$multi_dis.factor[is.na(both.sets$multi_dis.factor)]<-"Single domain expertise" #(setting is NA to non multidisciplinary)
###analyses with domain
model.bothTournaments.COVs<-lmer(inaccuracy_log~domain+parameters_coded+Method.complex+covidconditional+CounterFactual_Presence_Final+n_domains+team_discipline.datasci+team_discipline.SBsci+Multidisciplinary+team_size.coded+team_education+confidence+subexpert+Domain_Publications+previous_tournament.coded+(1|team_name), data=both.sets)
car::Anova(model.bothTournaments.COVs,type="III",test.statistic="F")
summ(model.bothTournaments.COVs, conf.method="boot", digits=5, center=T)
#Rsq = 0.31437
model.bothTournaments.no.COVs<-lmer(inaccuracy_log~domain+(1|team_name), data=both.sets)
summ(model.bothTournaments.no.COVs, conf.method="boot", digits=3, center=T)
#xtra analysis with US residents on the team - not included to avoi doverfitting.
model.bothTournaments.COVs.incl.US<-lmer(inaccuracy_log~domain+parameters_coded+Method.complex+covidconditional+CounterFactual_Presence_Final+n_domains+team_discipline.datasci+team_discipline.SBsci+Multidisciplinary+non_US+team_size.coded+team_education+confidence+subexpert+Domain_Publications+previous_tournament.coded+(1|team_name), data=both.sets)
car::Anova(model.bothTournaments.COVs.incl.US,type="III",test.statistic="F")
#no effect of US residency
#xtra analysis without objective expertise to examine partial Rsq
model.bothTournaments.COVs.no.obj.expertise<-lmer(inaccuracy_log~domain+parameters_coded+Method.complex+covidconditional+CounterFactual_Presence_Final+n_domains+team_discipline.datasci+team_discipline.SBsci+Multidisciplinary+team_size.coded+team_education+confidence+subexpert+previous_tournament.coded+(1|team_name), data=both.sets)
car::Anova(model.bothTournaments.COVs.no.obj.expertise,type="III",test.statistic="F")
summ(model.bothTournaments.COVs.no.obj.expertise, conf.method="boot", digits=5, center=T)
#Rsq - 0.30449
anova(model.bothTournaments.COVs,model.bothTournaments.COVs.no.obj.expertise)
#flip accuracy and inaccuracy
both.sets$accuracy_log<-both.sets$inaccuracy_log*(-1)
model.bothTournaments.accuracy.COVs<-lmer(accuracy_log~domain+parameters_coded+Method.complex+covidconditional+CounterFactual_Presence_Final+n_domains+team_discipline.datasci+team_discipline.SBsci+Multidisciplinary+team_size.coded+team_education+confidence+subexpert+Domain_Publications+previous_tournament.coded+(1|team_name), data=both.sets)
car::Anova(model.bothTournaments.accuracy.COVs,type="III",test.statistic="F")
summ(model.bothTournaments.accuracy.COVs, conf.method="boot", digits=3, center=T)
plot.COV<-plot_summs(model.bothTournaments.accuracy.COVs, scale = TRUE, robust = "HC1",n.sd = 2, inner_ci_level = .9,
coefs = c("Statistical Model Complexity" = "Method.complex","N Model Parameters" = "parameters_coded",
"Considered COVID-19" = "covidconditional",
"Considered Counterfactuals"="CounterFactual_Presence_Final",
"Number of Predicted Domains"="n_domains",
"Data Scientists on the Team"="team_discipline.datasci",
"Behav./Soc. Scientists on the Team"="team_discipline.SBsci",
"Multidisciplinary"="Multidisciplinary",
"Team Size"="team_size.coded",
"% without PhD on the Team"="team_education",
"Confidence in Forecast"="confidence",
"Self-report Expertise"="subexpert",
"Team Members Topic Publications"="Domain_Publications",
"Prev. Exp. with\nForecasting Tournaments"="previous_tournament.coded"))
plot.COV$data<-plot.COV$data %>% arrange(estimate) %>%    # First sort by val. This sort the dataframe but NOT the factor levels
mutate(term=factor(term, levels=term))
plot.COV+theme_pubclean()+labs(y="",x="Contribution to Accuracy",caption = "most negative <===========================================> most positive")
export_summs(model.bothTournaments.accuracy.COVs, scale = TRUE, robust = "HC1",n.sd = 2, to.file = "docx", file.name="indiv.differences.standartized.docx")
export_summs(model.bothTournaments.accuracy.COVs, scale = F, robust = "HC1", to.file = "docx", file.name="indiv.differences.unstandartized.docx")
partr2.COV<-partR2(model.bothTournaments.accuracy.COVs,  data=both.sets,
R2_type = "marginal", nboot = 10, CI = 0.95)
summary(partr2.COV) #obtain effect sizes scores for unique predictors (incremental R2)
partR2(model.bothTournaments.accuracy.COVs, partvars =
c("Domain_Publications","previous_tournament.coded","Method.complex","Multidisciplinary"))
##supplementary  - examine COVID score inaccuracy as predictor - does inaccuracy in predictions depend on COVID-inaccuracy?
#IMPORTANT: only done in Phase 1
phase1_exp$inaccuracy_log<-log(phase1_exp$MASE1_w1)
model.t1.COVID<-lmer(inaccuracy_log~domain+log(MASE1_covid)+(1|team_name), data=phase1_exp)
car::Anova(model.t1.COVID,type="III",test.statistic="F")
summ(model.t1.COVID, conf.method="boot", digits=3, center=T) #no significant effect of COVID prediction in accuracy on MASE in accuracy
names9both.sets
names(both.sets)
both.sets$phase
model.bothTournaments.accuracy.COVs.phase<-lmer(accuracy_log~phase+domain+parameters_coded+phase*Method.complex+covidconditional+CounterFactual_Presence_Final+n_domains+team_discipline.datasci+team_discipline.SBsci+Multidisciplinary+team_size.coded+team_education+confidence+subexpert+Domain_Publications+previous_tournament.coded+(1|team_name), data=both.sets)
car::Anova(model.bothTournaments.accuracy.COVs.phase,type="III",test.statistic="F")
summ(model.bothTournaments.accuracy.COVs.phase, conf.method="boot", digits=3, center=T)
model.bothTournaments.accuracy.COVs.phase<-lmer(accuracy_log~phase+domain+phase*parameters_coded+phase*Method.complex+covidconditional+CounterFactual_Presence_Final+n_domains+team_discipline.datasci+team_discipline.SBsci+Multidisciplinary+team_size.coded+team_education+confidence+subexpert+Domain_Publications+previous_tournament.coded+(1|team_name), data=both.sets)
car::Anova(model.bothTournaments.accuracy.COVs.phase,type="III",test.statistic="F") #no significant interaction between phase and complexity
